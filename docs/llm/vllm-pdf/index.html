<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.27.1 by Michael Rose
  Copyright 2013-2025 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>vLLM에 pdf 넣는법 - ARCHIVE</title>
<meta name="description" content="">


  <meta name="author" content="hvppyarchive">
  
  <meta property="article:author" content="hvppyarchive">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="ARCHIVE">
<meta property="og:title" content="vLLM에 pdf 넣는법">
<meta property="og:url" content="https://hvppyarchive.github.io/llm/vllm-pdf/">


  <meta property="og:description" content="">







  <meta property="article:published_time" content="2025-11-04T14:13:34+09:00">





  

  


<link rel="canonical" href="https://hvppyarchive.github.io/llm/vllm-pdf/">












<!-- end _includes/seo.html -->


<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->
<!-- hvppyarchive: add fonts -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Serif+KR:wght@200..900&display=swap" rel="stylesheet">

<!-- insert favicons. use https://realfavicongenerator.net/ -->
 <!-- hvppyarchive: add favicons -->
<link rel="icon" type="image/png" href="/assets/favicon/favicon-96x96.png" sizes="96x96" />
<link rel="icon" type="image/svg+xml" href="/assets/favicon/favicon.svg" />
<link rel="shortcut icon" href="/assets/favicon/favicon.ico" />
<link rel="apple-touch-icon" sizes="180x180" href="/assets/favicon/apple-touch-icon.png" />
<link rel="manifest" href="/assets/favicon/site.webmanifest" />

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          ARCHIVE
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/year-archive/"
                
                
              >Posts</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="vLLM에 pdf 넣는법">
    <meta itemprop="description" content="">
    <meta itemprop="datePublished" content="2025-11-04T14:13:34+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://hvppyarchive.github.io/llm/vllm-pdf/" itemprop="url">vLLM에 pdf 넣는법
</a>
          </h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <!-- hvppyarchive: remove date icon
        <i class="far fa-calendar-alt" aria-hidden="true"></i> 
        -->
        
        <time datetime="2025-11-04T14:13:34+09:00">2025-11-04</time>
      </span>
    

    

    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
        <p class="chat--user">

vllm 서빙하는 모델 gptoss에 pdf 넣는법

</p>

<p>요약 먼저: vLLM으로 서빙 중인 gpt-oss는 “PDF 파일을 직접 넣어 처리”하는 인터페이스가 없습니다. vLLM OpenAI 호환 서버는 텍스트/토큰 스트림을 받는 API이고, gpt-oss는 공개된 가이드 기준 텍스트 전용 모델이라서(이미지·PDF 업로드 파라미터 없음) PDF는 사전 전처리(텍스트 추출→청소→청크→검색/RAG)로 넣어야 합니다. <a href="https://docs.vllm.ai/en/v0.6.0/serving/openai_compatible_server.html?utm_source=chatgpt.com">docs.vllm.ai+4</a></p>

<p>아래는 “PDF를 gpt-oss(vLLM 서버)로 넣는” 가장 흔한 구현 레시피입니다. 단계별로 그대로 따라하면 됩니다.</p>

<ol>
  <li>vLLM 서버로 gpt-oss 띄우기</li>
</ol>

<ul>
  <li>CUDA 환경에서 예시:</li>
</ul>

<div class="language-css highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">pip</span> <span class="nt">install</span> <span class="nt">vllm</span>
<span class="nt">python</span> <span class="nt">-m</span> <span class="nt">vllm</span><span class="nc">.entrypoints.openai.api_server</span> <span class="err">\</span>
  <span class="nt">--model</span> <span class="nt">openai</span><span class="o">/</span><span class="nt">gpt-oss-20b</span> <span class="err">\</span>
  <span class="nt">--max-model-len</span> <span class="err">32768</span> <span class="err">\</span>
  <span class="nt">--port</span> <span class="err">8000</span>
</code></pre></div></div>

<p>서버는 OpenAI 호환 Chat/Completions 엔드포인트를 제공합니다. 기본적으로 파일 업로드 필드는 없고 문자열 프롬프트만 받습니다. <a href="https://docs.vllm.ai/en/v0.6.0/serving/openai_compatible_server.html?utm_source=chatgpt.com">docs.vllm.ai</a></p>

<ol>
  <li>PDF → 텍스트 추출</li>
</ol>

<ul>
  <li>
    <p>Python 도구 예: pypdf, pdfminer.six, unstructured, docling 등.</p>
  </li>
  <li>
    <p>표·머리글·각주가 섞여 있으면 docling/Unstructured 계열이 품질이 더 낫습니다. 추출 후 다음을 꼭 하세요.<br />
• 페이지 헤더/푸터 제거, 공백 정규화, 본문 병합<br />
• 너무 긴 표는 CSV로 따로 저장(필요시)<br />
관련 이슈와 파이프라인 선택은 커뮤니티/라이브러리 이슈를 참고하세요. <a href="https://github.com/docling-project/docling/issues/1365?utm_source=chatgpt.com">GitHub</a></p>
  </li>
</ul>

<ol>
  <li>청크 나누기</li>
</ol>

<ul>
  <li>
    <p>800~1500자(또는 200~400 토큰) 단위로 자르고, 문단 경계를 최대한 유지합니다.</p>
  </li>
  <li>
    <p>각 청크에 원문 페이지/섹션 메타데이터를 붙입니다.</p>
  </li>
</ul>

<ol>
  <li>임베딩 생성 → 벡터DB 적재</li>
</ol>

<ul>
  <li>
    <p>임베딩 모델(예: bge-m3, jina-embeddings, sentence-transformers 등)로 각 청크를 벡터화하고, FAISS/Chroma/PGVector에 넣습니다.</p>
  </li>
  <li>
    <p>이 단계는 LLM(gpt-oss/vLLM)과 분리된 별도 과정입니다. vLLM 자체는 임베딩을 만들지 않습니다. RAG 패턴 예시는 다수의 vLLM 배포 예제에서도 동일합니다. <a href="https://validatedpatterns.io/patterns/rag-llm-gitops/?utm_source=chatgpt.com">validatedpatterns.io+1</a></p>
  </li>
</ul>

<ol>
  <li>질의 시 검색(RAG) → 프롬프트 구성 → vLLM에 전달</li>
</ol>

<ul>
  <li>
    <p>유저 질문을 벡터 검색으로 top-k 청크와 매칭</p>
  </li>
  <li>
    <p>아래 System/Assistant 컨텍스트에 “검색 결과”를 넣고, gpt-oss의 권장 포맷(하모니 포맷)을 지키며 요청을 보냅니다. gpt-oss는 OpenAI 호환 서버에서 잘 동작하도록 가이드가 있습니다. <a href="https://cookbook.openai.com/articles/gpt-oss/run-vllm?utm_source=chatgpt.com">cookbook.openai.com+1</a></p>
  </li>
</ul>

<p>실전 코드 스니펫</p>

<p>A. PDF 텍스트 추출/청크(간단 예시)</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pypdf</span> <span class="kn">import</span> <span class="n">PdfReader</span>

<span class="k">def</span> <span class="nf">extract_pdf_text</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
    <span class="n">reader</span> <span class="o">=</span> <span class="nc">PdfReader</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">pages</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">reader</span><span class="p">.</span><span class="n">pages</span><span class="p">):</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="nf">extract_text</span><span class="p">()</span> <span class="ow">or</span> <span class="sh">""</span>
        <span class="n">pages</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">page</span><span class="sh">"</span><span class="p">:</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="nf">split</span><span class="p">())})</span>
    <span class="k">return</span> <span class="n">pages</span>

<span class="k">def</span> <span class="nf">chunk_text</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">max_chars</span><span class="o">=</span><span class="mi">1200</span><span class="p">,</span> <span class="n">overlap</span><span class="o">=</span><span class="mi">150</span><span class="p">):</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">start</span> <span class="o">=</span> <span class="p">[],</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
        <span class="n">end</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">text</span><span class="p">),</span> <span class="n">start</span> <span class="o">+</span> <span class="n">max_chars</span><span class="p">)</span>
        <span class="n">out</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">])</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">overlap</span>
        <span class="k">if</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span> <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="n">pages</span> <span class="o">=</span> <span class="nf">extract_pdf_text</span><span class="p">(</span><span class="sh">"</span><span class="s">sample.pdf</span><span class="sh">"</span><span class="p">)</span>
<span class="n">chunks</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">page</span> <span class="ow">in</span> <span class="n">pages</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nf">chunk_text</span><span class="p">(</span><span class="n">page</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">]):</span>
        <span class="n">chunks</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">page</span><span class="sh">"</span><span class="p">:</span> <span class="n">page</span><span class="p">[</span><span class="sh">"</span><span class="s">page</span><span class="sh">"</span><span class="p">],</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">c</span><span class="p">})</span>
</code></pre></div></div>

<p>B. 임베딩→FAISS 적재(예시: sentence-transformers)</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>
<span class="kn">import</span> <span class="n">faiss</span><span class="p">,</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">SentenceTransformer</span><span class="p">(</span><span class="sh">"</span><span class="s">sentence-transformers/all-MiniLM-L6-v2</span><span class="sh">"</span><span class="p">)</span>
<span class="n">emb</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encode</span><span class="p">([</span><span class="n">c</span><span class="p">[</span><span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">],</span> <span class="n">convert_to_numpy</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">normalize_embeddings</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">index</span> <span class="o">=</span> <span class="n">faiss</span><span class="p">.</span><span class="nc">IndexFlatIP</span><span class="p">(</span><span class="n">emb</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">index</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">emb</span><span class="p">)</span>
</code></pre></div></div>

<p>C. 검색→vLLM(gpt-oss) 호출(OpenAI 호환)</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span><span class="p">,</span> <span class="n">requests</span><span class="p">,</span> <span class="n">json</span>

<span class="k">def</span> <span class="nf">search</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">topk</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encode</span><span class="p">([</span><span class="n">query</span><span class="p">],</span> <span class="n">convert_to_numpy</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">normalize_embeddings</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">scores</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">index</span><span class="p">.</span><span class="nf">search</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">topk</span><span class="p">)</span>
    <span class="n">ctx</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">ctx</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">[p.</span><span class="si">{</span><span class="n">chunks</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="sh">'</span><span class="s">page</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="s">]</span><span class="se">\n</span><span class="si">{</span><span class="n">chunks</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="sh">'</span><span class="s">content</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="sh">"</span><span class="se">\n\n</span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">ask_vllm</span><span class="p">(</span><span class="n">query</span><span class="p">):</span>
    <span class="n">context</span> <span class="o">=</span> <span class="nf">search</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">topk</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span><span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span><span class="sh">"</span><span class="s">You are a helpful assistant. Use the provided CONTEXT; if missing, say you don</span><span class="sh">'</span><span class="s">t know.</span><span class="sh">"</span><span class="p">},</span>
        <span class="p">{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span><span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span><span class="sa">f</span><span class="sh">"</span><span class="s">CONTEXT:</span><span class="se">\n</span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="se">\n\n</span><span class="s">QUESTION:</span><span class="se">\n</span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="sh">"</span><span class="p">}</span>
    <span class="p">]</span>
    <span class="n">resp</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">post</span><span class="p">(</span>
        <span class="sh">"</span><span class="s">http://localhost:8000/v1/chat/completions</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">headers</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">Content-Type</span><span class="sh">"</span><span class="p">:</span><span class="sh">"</span><span class="s">application/json</span><span class="sh">"</span><span class="p">},</span>
        <span class="n">data</span><span class="o">=</span><span class="n">json</span><span class="p">.</span><span class="nf">dumps</span><span class="p">({</span><span class="sh">"</span><span class="s">model</span><span class="sh">"</span><span class="p">:</span><span class="sh">"</span><span class="s">openai/gpt-oss-20b</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">messages</span><span class="sh">"</span><span class="p">:</span><span class="n">messages</span><span class="p">,</span><span class="sh">"</span><span class="s">temperature</span><span class="sh">"</span><span class="p">:</span><span class="mf">0.2</span><span class="p">})</span>
    <span class="p">).</span><span class="nf">json</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">resp</span><span class="p">[</span><span class="sh">"</span><span class="s">choices</span><span class="sh">"</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">message</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">]</span>

<span class="nf">print</span><span class="p">(</span><span class="nf">ask_vllm</span><span class="p">(</span><span class="sh">"</span><span class="s">이 PDF에서 반품 정책 핵심 요약해줘</span><span class="sh">"</span><span class="p">))</span>
</code></pre></div></div>

<p>D. curl로 간단 테스트</p>
<div class="language-swift highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">curl</span> <span class="nv">http</span><span class="p">:</span><span class="c1">//localhost:8000/v1/chat/completions \</span>
  <span class="o">-</span><span class="kt">H</span> <span class="s">"Content-Type: application/json"</span> <span class="p">\</span>
  <span class="o">-</span><span class="n">d</span> <span class="err">'</span><span class="p">{</span>
    <span class="s">"model"</span><span class="p">:</span> <span class="s">"openai/gpt-oss-20b"</span><span class="p">,</span>
    <span class="s">"messages"</span><span class="p">:</span> <span class="p">[</span>
      <span class="p">{</span><span class="s">"role"</span><span class="p">:</span><span class="s">"system"</span><span class="p">,</span><span class="s">"content"</span><span class="p">:</span><span class="s">"Answer using the context."</span><span class="p">},</span>
      <span class="p">{</span><span class="s">"role"</span><span class="p">:</span><span class="s">"user"</span><span class="p">,</span><span class="s">"content"</span><span class="p">:</span><span class="s">"CONTEXT:</span><span class="se">\n</span><span class="s">&lt;여기에 검색으로 모은 PDF 청크&gt;</span><span class="se">\n\n</span><span class="s">QUESTION:</span><span class="se">\n</span><span class="s">요약해줘"</span><span class="p">}</span>
    <span class="p">],</span>
    <span class="s">"temperature"</span><span class="p">:</span> <span class="mf">0.2</span>
  <span class="p">}</span><span class="err">'</span>
</code></pre></div></div>

<p>자주 묻는 포인트</p>

<ul>
  <li>
    <p>왜 PDF 파일 자체를 바로 못 넣나?<br />
vLLM의 OpenAI 호환 서버는 파일 업로드/스토리지 기능이 아니라 텍스트 프롬프트 API입니다. 즉, 파일은 애플리케이션 레이어에서 처리해서 텍스트(또는 검색된 컨텍스트)로 바꾼 다음 모델에 넣어야 합니다. <a href="https://docs.vllm.ai/en/v0.6.0/serving/openai_compatible_server.html?utm_source=chatgpt.com">docs.vllm.ai</a></p>
  </li>
  <li>
    <p>gpt-oss가 PDF나 이미지까지 직접 읽는 멀티모달인가?<br />
공개 가이드 기준 gpt-oss는 텍스트 중심의 오픈 웨이트 패밀리로 소개되었습니다. 멀티모달 입력을 직접 받는 형태가 아니라, 텍스트로 컨텍스트를 주는 방식이 표준입니다. <a href="https://openai.com/index/introducing-gpt-oss/?utm_source=chatgpt.com">OpenAI+2</a></p>
  </li>
  <li>
    <p>“PDF 질의응답” 정밀도를 더 높이려면?<br />
청크 크기/오버랩 최적화, 표/코드 블록 보존, 섹션별 재순위화(re-ranking), “출처 페이지 표시”를 프롬프트에 강제, 그리고 답변 근거를 따로 출력하도록 규칙을 넣으면 품질이 안정됩니다. vLLM 레시피/쿠크북의 gpt-oss 운영 가이드도 함께 보세요. <a href="https://docs.vllm.ai/projects/recipes/en/latest/OpenAI/GPT-OSS.html?utm_source=chatgpt.com">docs.vllm.ai+1</a></p>
  </li>
</ul>

<p>한 줄 정리: vLLM+gpt-oss로 PDF를 “넣는” 것은 PDF를 텍스트로 전처리하고, 검색 기반(RAG)으로 관련 청크를 추려 OpenAI 호환 API에 문자열로 보내는 것입니다. 이 흐름을 코드와 같이 구성하면 바로 운영에 올릴 수 있습니다.</p>

<p class="chat--user">

openai agent 라는 패키지가 있는데 여기에 그런 레이어가 포함되어있나?

</p>

<p>짧은 결론부터 말하면, “openai agent”로 알려진 OpenAI Agents SDK는 에이전트 오케스트레이션(툴 호출, 세션/메모리, 핸드오프, 가드레일 등)을 제공하지만 PDF 파싱·청크·임베딩·벡터스토어 적재 같은 “문서 RAG 인입 레이어”는 기본 내장되어 있지 않습니다. 다만 OpenAI 모델을 쓸 때는 호스티드 도구(FileSearchTool 등)를 통해 OpenAI Vector Stores에 올려둔 문서를 검색하는 방식은 지원합니다. 자체 vLLM+gpt-oss로 돌릴 때는 그 레이어를 직접 붙여야 합니다. <a href="https://openai.github.io/openai-agents-python/">openai.github.io+1</a></p>

<p>핵심 포인트를 정리하면 아래와 같습니다.</p>

<ol>
  <li>
    <p>SDK가 해주는 일<br />
에이전트 루프, 함수툴(FunctionTool), 세션/트레이싱, 핸드오프, MCP(Model Context Protocol) 연결 등 “오케스트레이션”이 주역할입니다. PDF를 읽어 텍스트로 만들고 임베딩을 뽑아 벡터DB에 적재하는 파이프라인은 앱 쪽에서 구성해야 합니다. <a href="https://openai.github.io/openai-agents-python/">openai.github.io+1</a></p>
  </li>
  <li>
    <p>OpenAI 호스티드 도구가 있을 때<br />
OpenAI Responses 모델을 쓰면 FileSearchTool, WebSearchTool, Code Interpreter 같은 “호스티드 툴”을 바로 붙일 수 있습니다. 이 경우 PDF는 OpenAI Vector Stores API로 업로드·처리하고, 에이전트는 FileSearchTool로 검색합니다. SDK 자체가 PDF를 파싱해주진 않고, 벡터스토어도 OpenAI 쪽 리소스를 이용하는 구성입니다. <a href="https://openai.github.io/openai-agents-python/tools/">openai.github.io</a></p>
  </li>
  <li>
    <p>gpt-oss(+vLLM)와 함께 쓰는 경우<br />
vLLM의 OpenAI-호환 엔드포인트로 에이전트의 모델 클라이언트를 바꿔치기 할 수 있습니다. 다만 이때는 OpenAI 호스티드 도구(FileSearchTool 등)가 그대로 동작하지 않거나 전제가 바뀌므로, 직접 함수툴로 “검색/RAG”를 구현하거나 MCP 서버(자체 문서 검색 서버)를 붙여서 문서 검색을 노출하는 패턴이 일반적입니다. <a href="https://cookbook.openai.com/articles/gpt-oss/run-vllm?utm_source=chatgpt.com">cookbook.openai.com+2</a></p>
  </li>
  <li>
    <p>최소 구현 스케치</p>
  </li>
</ol>

<ul>
  <li>
    <p>PDF 인입: pypdf/unstructured로 텍스트 추출 → 청크 → 임베딩(bge/jina/sentence-transformers) → FAISS/Chroma 적재</p>
  </li>
  <li>
    <p>에이전트 툴: search(query) 같은 Python 함수툴을 정의해 top-k 청크를 반환하도록 구현</p>
  </li>
  <li>
    <p>에이전트: Agent(tools=[search])로 등록, 모델은 vLLM의 Chat Completions/Responses 호환 엔드포인트로 지정</p>
  </li>
  <li>
    <p>OpenAI 모델로 바꿀 때만 FileSearchTool을 사용해 OpenAI Vector Store를 조회 가능<br />
이 구성은 SDK 문서의 “Tools”, “Context management”, “MCP” 섹션 흐름과 딱 맞습니다. <a href="https://openai.github.io/openai-agents-python/tools/">openai.github.io+2</a></p>
  </li>
</ul>

<ol>
  <li>이름 혼동 주의<br />
Assistants API는 Responses API로 대체되는 흐름이며, Agents SDK는 이 Responses 기반에서 돌아갑니다. 최신 가이드와 레퍼런스를 참고해서 구성하세요. <a href="https://www.reuters.com/technology/artificial-intelligence/openai-launches-new-developer-tools-chinese-ai-startups-gain-ground-2025-03-11/?utm_source=chatgpt.com">reuters.com+1</a></li>
</ol>

<p>정리</p>

<ul>
  <li>포함되어 있는가? 에이전트 오케스트레이션은 포함, “PDF→RAG 인입 레이어”는 미포함. OpenAI 모델일 때만 FileSearchTool로 OpenAI Vector Stores를 조회할 수 있음. vLLM+gpt-oss에서는 직접 함수툴이나 MCP로 붙여야 합니다. <a href="https://openai.github.io/openai-agents-python/tools/">openai.github.io+1</a></li>
</ul>

<p>원하시면 지금 쓰시는 vLLM 서버 주소와 선호 임베딩/벡터DB를 가정해, Agents SDK용 최소 예제(함수툴 기반 검색 + vLLM 연결) 스니펫을 바로 만들어 드립니다.</p>

<p class="chat--user">

openai vector store는 어떤식으로 구성되어있어?

</p>

<p>좋아요 — “OpenAI Vector Store”가 대체로 어떻게 구성되어 있는지(=아키텍처, 주요 컴포넌트, API 흐름) 같이 살펴봅시다. (약간 기술적이긴 하지만, 전체 그림을 이해하면 이후 구현할 때 훨씬 수월해요.)</p>

<hr />

<h2 id="1-기본-개념">1. 기본 개념</h2>

<p>Vector Store는 간단히 말해 “문서/청크(텍스트 단위)를 임베딩 벡터로 변환해 저장하고, 이후 검색 가능하도록 조회하는 저장소”입니다. OpenAI 쪽에서 제공하는 API는 문서를 업로드 → 임베딩/청크화(내부 처리) → 벡터저장소 생성 및 파일 바인딩 → 검색 질의(Query) → 관련 청크 반환하는 흐름을 지원합니다. <a href="https://platform.openai.com/docs/assistants/tools/file-search/vector-stores?utm_source=chatgpt.com">platform.openai.com+3</a></p>

<p>즉, 내부적으로는 다음 요소들이 있습니다:</p>

<ul>
  <li>
    <p>문서 파일(예: PDF, 텍스트) 업로드 및 처리된 파일 배치(file batch)</p>
  </li>
  <li>
    <p>벡터 변환(임베딩) + 청크(metadata 포함)</p>
  </li>
  <li>
    <p>저장소(Vector store) 객체: 여러 파일/청크들을 묶음</p>
  </li>
  <li>
    <p>검색 API: 벡터 유사도 기반 조회 → 반환된 청크와 메타정보</p>
  </li>
</ul>

<hr />

<h2 id="2-openai-api-상의-구성-요소">2. OpenAI API 상의 구성 요소</h2>

<p>OpenAI 문서를 기준으로 주요 리소스(resource)들이 어떻게 연결되는지요:</p>

<ul>
  <li>
    <p><strong>Vector Store 리소스</strong> : <code class="language-plaintext highlighter-rouge">/v1/vector_stores</code> 엔드포인트로 만들 수 있음. <a href="https://platform.openai.com/docs/api-reference/vector-stores?utm_source=chatgpt.com">platform.openai.com+1</a></p>
  </li>
  <li>
    <p><strong>파일 배치(File Batches)</strong> : 업로드된 파일이 청크/벡터화되어 저장되는 단위. <code class="language-plaintext highlighter-rouge">/v1/vector_stores/{vector_store_id}/file_batches</code> 같은 구조 있음. <a href="https://platform.openai.com/docs/api-reference/vector-stores-file-batches?utm_source=chatgpt.com">platform.openai.com</a></p>
  </li>
  <li>
    <p><strong>파일(파일 업로드)</strong> : <code class="language-plaintext highlighter-rouge">/v1/vector_stores/{vs_id}/files</code> 엔드포인트에서 파일을 추가할 수 있음. <a href="https://platform.openai.com/docs/api-reference/vector-stores-files?utm_source=chatgpt.com">platform.openai.com</a></p>
  </li>
  <li>
    <p><strong>검색(Search)</strong> : 저장된 Vector Store에 대해 “유사벡터 기반 검색”을 요청 가능 (예: 파일 검색, 청크 검색) <a href="https://platform.openai.com/docs/assistants/tools/file-search/vector-stores?utm_source=chatgpt.com">platform.openai.com+1</a></p>
  </li>
</ul>

<hr />

<h2 id="3-흐름-예시로-보기">3. 흐름 예시로 보기</h2>

<p>아래처럼 단계별 구성됩니다:</p>

<ol>
  <li>
    <p>파일 업로드</p>

    <ul>
      <li>예: PDF를 업로드 → OpenAI 측에서 자동으로 청크화/임베딩/저장 준비 → 파일 배치 생성</li>
    </ul>
  </li>
  <li>
    <p>Vector Store 생성</p>

    <ul>
      <li>저장소 이름, 설명 등의 메타데이터 제공 → 생성</li>
    </ul>
  </li>
  <li>
    <p>파일을 Vector Store에 바인딩(=파일 추가)</p>

    <ul>
      <li>생성한 Vector Store ID에 파일 배치/파일을 업로드하는 API 호출</li>
    </ul>
  </li>
  <li>
    <p>검색 요청</p>

    <ul>
      <li>
        <p>사용자가 질의(query)를 보내면, API는 임베딩 생성 → 저장된 벡터들과 유사도 비교 → 관련 청크 반환</p>
      </li>
      <li>
        <p>반환된 청크를 기반으로 응답 생성(LLM 호출 전처리 역할)</p>
      </li>
    </ul>
  </li>
  <li>
    <p>LLM 호출(별도)</p>

    <ul>
      <li>검색된 청크는 LLM(예: gpt-모델)에 컨텍스트로 전달 → 답변 생성</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="4-주요-특성-및-한계">4. 주요 특성 및 한계</h2>

<ul>
  <li>
    <p>파일/문서 메타데이터는 비교적 제한적: 업로드된 파일이 얼마나 긴지(token count, char count) 등의 정보 부족하다는 사용자의 피드백이 있음. <a href="https://www.reddit.com/r/OpenAI/comments/1lacgqe/openais_vector_store_api_is_missing_basic/?utm_source=chatgpt.com">Reddit</a></p>
  </li>
  <li>
    <p>청크화 + 임베딩 + 저장/관리 과정을 “사용자제어” 하려면 OpenAI 호스티드 기능보다는 자체 벡터DB + 임베딩 워크플로우가 더 유연함</p>
  </li>
  <li>
    <p>검색 품질은 임베딩 모델(어떤 모델을 썼는가) + 청크 단위/문서 구조 설계에 크게 좌우됨</p>
  </li>
</ul>

<hr />

<h2 id="5-gpt-ossvllm-활용-시-고려사항">5. gpt-oss/vLLM 활용 시 고려사항</h2>

<p>당신이 vLLM + gpt-oss 조합으로 쓰는 상황이라면, OpenAI Vector Store API를 그대로 의존하기보다는 다음과 같은 변형이 필요할 수 있어요:</p>

<ul>
  <li>
    <p>OpenAI Vector Store API를 쓰고 싶으면 OpenAI 인증/요금/호스팅 구조 따라야 함</p>
  </li>
  <li>
    <p>또는 자체 벡터DB(예: FAISS, Milvus 등) + 임베딩 모델 + 검색 함수툴을 만들어서 “유사 기능”을 구현</p>
  </li>
  <li>
    <p>Agents SDK나 워크플로우에서는 “검색 툴(tool)”이 이 저장소와 연결됨: 사용자가 질의하면 검색툴이 관련 청크를 꺼내오고, LLM에게 문맥으로 제공</p>
  </li>
</ul>

<hr />

<p>필요하시다면, OpenAI Vector Store API를 실제로 호출하는 예제 코드(Python) 만들어 드릴 수 있어요—특히 파일 업로드 → 검색까지 흐름. 그렇게 할까요?</p>

<p class="chat--user">

마지막에 말한 검색툴은 자동으로 동작하는건지 llm이 요청하면 동작하는건지 궁금해

</p>

<p>아주 핵심을 짚으셨습니다.<br />
OpenAI의 “검색 툴(FileSearchTool)”은 <strong>자동으로 동작하지 않습니다</strong>.<br />
LLM이 “필요하다고 판단할 때” — 즉, <strong>모델이 툴 호출을 생성하면</strong> — 그때 실행됩니다.</p>

<p>이걸 조금 더 세밀하게 뜯어보면 이렇게 됩니다.</p>

<hr />

<h3 id="1-에이전트의-기본-구조">1. 에이전트의 기본 구조</h3>

<p>OpenAI Agents SDK(혹은 Responses API)에서 에이전트는 다음 두 가지를 갖습니다.</p>

<ul>
  <li>
    <p><strong>LLM 모델</strong> : gpt-4o, gpt-4o-mini 등</p>
  </li>
  <li>
    <p><strong>툴(tool)</strong> : 함수 호출(Function Tool), FileSearchTool, WebSearchTool, CodeInterpreter 등</p>
  </li>
</ul>

<p>에이전트는 LLM에게 “너는 이런 툴들을 쓸 수 있다”고 알려주고,<br />
대화 중에 LLM이 판단해 특정 툴을 호출할 때만 실제 실행 로직이 수행됩니다.</p>

<hr />

<h3 id="2-툴-호출-과정">2. 툴 호출 과정</h3>

<p>모델이 내부적으로 이런 구조의 응답을 냅니다.</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"function_call"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"search"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"arguments"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"query"</span><span class="p">:</span><span class="w"> </span><span class="s2">"계약서 요약"</span><span class="w"> </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>SDK는 이걸 감지하고 <strong>search() 함수</strong> (혹은 FileSearchTool)을 호출합니다.<br />
그 결과를 받아 모델에게 다시 전달하면, 모델은 그 결과를 바탕으로 <strong>최종 답변</strong> 을 만듭니다.</p>

<p>즉, LLM이 툴 사용을 “결정”하고, 호출은 SDK가 “대리 실행”합니다.</p>

<hr />

<h3 id="3-자동성과-자율성의-정도">3. 자동성과 자율성의 정도</h3>

<ul>
  <li>
    <p><strong>자동 실행 아님</strong> : 사용자가 “검색을 꼭 하라”고 지정하지 않으면, 모델이 문맥상 필요하다고 판단해야 호출합니다.</p>
  </li>
  <li>
    <p><strong>프롬프트 설계 가능</strong> : system prompt 에 “파일 내용을 참조할 때는 항상 FileSearchTool을 사용하라”처럼 명시해 주면 호출 확률이 높아집니다.</p>
  </li>
  <li>
    <p><strong>Agent Loop</strong> : SDK가 이 과정을 자동 반복해줍니다. 모델이 툴 호출 → 실행 → 결과 전달 → 다시 모델 호출 … 최종 답변이 완성될 때까지 루프를 돕습니다.</p>
  </li>
</ul>

<hr />

<h3 id="4-예시로-보는-흐름">4. 예시로 보는 흐름</h3>

<p>사용자: “이 계약서 요약해줘.”</p>

<ol>
  <li>
    <p>모델은 FileSearchTool이 등록되어 있음을 알고 있음.</p>
  </li>
  <li>
    <p>LLM이 내부적으로 판단: “파일에서 내용 가져와야 하네.”</p>
  </li>
  <li>
    <p>LLM → <code class="language-plaintext highlighter-rouge">function_call: file_search(query="계약서 요약")</code></p>
  </li>
  <li>
    <p>SDK → 실제 Vector Store 검색 수행</p>
  </li>
  <li>
    <p>검색 결과를 모델에게 다시 입력</p>
  </li>
  <li>
    <p>모델 → 검색 결과를 요약한 최종 답변 생성</p>
  </li>
</ol>

<hr />

<h3 id="5-vllm--gpt-oss-조합에서는">5. vLLM + gpt-oss 조합에서는</h3>

<p>vLLM은 OpenAI의 Responses API 내부 오케스트레이션 기능이 없습니다.<br />
따라서 “자동 툴 호출”을 구현하려면 SDK가 필요합니다.<br />
직접 서버나 에이전트 루프를 만들고, 모델의 함수 호출 결과(JSON)를 해석해 수동으로 “search() → 결과 → 모델 재호출”을 구현해야 합니다.</p>

<hr />

<p>정리하자면:<br />
<strong>FileSearchTool은 모델이 필요하다고 판단할 때만 호출되고, SDK가 실행을 중계합니다.</strong><br />
즉, “모델이 요청하면 자동으로 동작한다”는 표현이 가장 정확합니다.</p>

<p>다음 단계로, 원하신다면 vLLM + gpt-oss 조합에서 이 “툴 호출 루프”를 흉내내는 최소 예제(모델 → function_call → search → 재호출)를 만들어 드릴 수 있습니다.</p>

<p class="chat--user">

영어로 작성된 문서 rag를 한글로 쿼리할 수 있어?

</p>

<p>가능합니다. 방법은 크게 두 가지 계열입니다. 한글 쿼리 → 영어 문서 RAG를 지원하려면 “다국어 임베딩”을 쓰거나 “번역 레이어”를 끼우면 됩니다.</p>

<ol>
  <li>다국어 임베딩으로 직접 검색</li>
</ol>

<ul>
  <li>
    <p>핵심: 영어 문서 청크와 한국어 쿼리를 같은 임베딩 공간으로 매핑.</p>
  </li>
  <li>
    <p>모델 예: bge-m3, e5-multilingual, LaBSE, mContriever 등.</p>
  </li>
  <li>
    <p>장점: 번역 없이도 개념 맞춤. 고유명사·전문용어에 강함.</p>
  </li>
  <li>
    <p>단점: 임베딩 품질·차원 수가 커서 메모리↑. 영어 전용 임베딩 대비 약간의 정밀도 손실이 있을 수 있음.</p>
  </li>
</ul>

<ol>
  <li>번역 후 검색(translate-then-retrieve)</li>
</ol>

<ul>
  <li>
    <p>파이프라인: 한글 쿼리 → 영어로 번역 → 영어 전용 임베딩(예: e5-large, bge-large-en 등)으로 검색 → 답변은 한글로 생성.</p>
  </li>
  <li>
    <p>장점: 영어 전용 임베딩/리랭커 품질을 그대로 활용 가능.</p>
  </li>
  <li>
    <p>단점: 번역 오류가 쿼리의 핵심 토픽을 비트는 경우가 있음. 엔티티(제품명, 표기) 주의.</p>
  </li>
</ul>

<p>권장 설계(현업 안정판)</p>

<ul>
  <li>
    <p>인덱싱: 영어 문서를 문단/문장 단위로 청크, 메타데이터(페이지, 섹션, 언어=en) 저장.</p>
  </li>
  <li>
    <p>임베딩: 다국어 임베딩 인덱스 1개는 반드시 구축. 여력이 되면 영어 전용 인덱스를 추가로 구성해 듀얼 인덱스 검색.</p>
  </li>
  <li>
    <p>검색 전략: 한글 쿼리에 대해</p>

    <ol>
      <li>
        <p>다국어 인덱스에서 top-k</p>
      </li>
      <li>
        <p>영어 전용 인덱스(쿼리 번역)에서도 top-k</p>
      </li>
      <li>
        <p>두 결과를 점수 정규화 후 병합</p>
      </li>
    </ol>
  </li>
  <li>
    <p>리랭킹: 다국어 리랭커(bge-reranker-v2-m3 같은 범용 다국어 리랭커)로 상위 50→10 재정렬.</p>
  </li>
  <li>
    <p>하이브리드: BM25(키워드) + 벡터 점수 가중 평균. 고유명사 매칭 강화.</p>
  </li>
  <li>
    <p>생성: gpt-oss(vLLM)에 “출처 표시, 영어 인용문은 원문 유지, 최종 답변은 한국어”를 시스템 규칙으로 주입.</p>
  </li>
</ul>

<p>간단 파이프라인 예시(핵심 로직만)</p>

<ul>
  <li>
    <p>인덱싱</p>

    <ol>
      <li>
        <p>PDF → 텍스트 추출 → 청크</p>
      </li>
      <li>
        <p>multilingual_embed(chunks) → FAISS/Chroma/PGVector에 저장</p>
      </li>
      <li>
        <p>선택: english_embed(chunks)로 영어 전용 인덱스도 병행 구축</p>
      </li>
    </ol>
  </li>
  <li>
    <p>질의</p>

    <ol>
      <li>
        <p>ko_query → multilingual_embed(ko_query)로 검색</p>
      </li>
      <li>
        <p>ko_query → en_query로 번역 → english_embed(en_query)로 검색</p>
      </li>
      <li>
        <p>결과 병합 → 다국어 리랭킹</p>
      </li>
      <li>
        <p>상위 n개를 컨텍스트로 vLLM에 넣고 한국어로 답변 생성</p>
      </li>
    </ol>
  </li>
</ul>

<p>프롬프트 팁</p>

<ul>
  <li>
    <p>시스템 규칙에 “문헌은 영어 원문을 그대로 인용, 설명은 한국어로 작성, 출처 페이지/섹션을 함께 적시”를 명시.</p>
  </li>
  <li>
    <p>다국어 혼용 문서일 경우 “언어 감지 → 쿼리와 다른 언어라도 동일 개념이면 포함” 같은 지침을 추가.</p>
  </li>
</ul>

<p>실무 체크리스트</p>

<ul>
  <li>
    <p>토크나이저 길이: 청크는 200~400토큰(문단 경계 유지).</p>
  </li>
  <li>
    <p>메타데이터: page, section, filename, lang. 답변에 근거 표시 용이.</p>
  </li>
  <li>
    <p>고유명사: 대소문자·하이픈·스네이크케이스 보존. 정규화 과도하게 하지 않기.</p>
  </li>
  <li>
    <p>캐싱: 빈번한 쿼리 번역/임베딩 결과 캐시.</p>
  </li>
  <li>
    <p>평가: 한-영 쌍으로 질의–정답 골드셋을 만들어 MRR@n, Hit@n, nDCG로 비교.</p>
  </li>
</ul>

<p>vLLM + gpt-oss 연결 포인트</p>

<ul>
  <li>
    <p>위 검색 결과(top-k 청크)를 user 메시지에 CONTEXT로 넣고 질문은 한국어로 전달.</p>
  </li>
  <li>
    <p>답변은 한국어로 강제, 필요한 부분만 영어 원문 인용.</p>
  </li>
  <li>
    <p>함수툴(예: search_docs)을 한 개 제공해 “모델이 필요 시 호출”하도록하면 자동 루프(에이전트 스타일)도 재현 가능.</p>
  </li>
</ul>

<p>요약</p>

<ul>
  <li>가능하고, 실전에서는 “다국어 임베딩 + 번역 병행 + 리랭킹”이 안정적입니다. 이미 구축된 영어 코퍼스에 최소 변경으로 한글 질의를 붙일 수 있고, gpt-oss 서빙과도 잘 맞습니다.</li>
</ul>

<p class="chat--user">

아 그리고 아까 search function을 만든다고 했잖아. 그 함수는 모든 RAG 문서를 다 뒤져 아니면 문서가 만약 100개 등록됐다고 치면 가장 관련된 10개만 추려서 10개에 대한 매칭만 해?

</p>

<p>짧게 핵심부터: 잘 만든 search 함수는 “전부 뒤지기”가 아니라 보통 2단계로 좁힙니다. 1단계에서 문서 레벨로 상위 N개를 고르고, 2단계에서 그 문서들 안에서 청크 레벨로 상위 K개만 뽑아 LLM에 넣습니다. 보편값은 N≈5~20, K≈3~8, 그리고 최종 컨텍스트 토큰 상한으로 컷을 겁니다.</p>

<p>아키텍처를 단계별로 정리해볼게요.</p>

<ol>
  <li>컬렉션과 인덱스</li>
</ol>

<ul>
  <li>
    <p>문서 100개가 등록되어 있다면 우선 전부를 하나의 벡터 인덱스(또는 컬렉션)로 관리합니다.</p>
  </li>
  <li>
    <p>문서 메타데이터(파일명, 섹션, 날짜, 언어, 권한, 태그)를 함께 저장해 필터링에 활용합니다.</p>
  </li>
  <li>
    <p>하이브리드 검색 권장: 벡터(의미) + BM25(키워드)를 함께 써서 고유명사·숫자 매칭을 보완합니다.</p>
  </li>
</ul>

<ol>
  <li>1단계: 문서 레벨 프리셀렉션</li>
</ol>

<ul>
  <li>
    <p>쿼리 임베딩과 문서 대표 벡터를 비교해 상위 N개 문서를 고릅니다.</p>
  </li>
  <li>
    <p>문서 대표 벡터는 보통 문서 제목+요약(abstract) 또는 상위 몇 개 청크의 평균으로 만듭니다.</p>
  </li>
  <li>
    <p>필터 예: lang=en, updated_at&gt;2025-01-01, tag in [“policy”, “spec”] 같은 조건을 먼저 걸어 탐색 공간을 줄입니다.</p>
  </li>
  <li>
    <p>다중 전략 결합: 벡터 상위 N1, BM25 상위 N2를 뽑아 Reciprocal Rank Fusion 같은 방식으로 합칩니다.</p>
  </li>
</ul>

<ol>
  <li>2단계: 청크 레벨 정밀 검색</li>
</ol>

<ul>
  <li>
    <p>선택된 N개 문서 각각에서 top_k_chunks_per_doc=K로 청크를 뽑습니다.</p>
  </li>
  <li>
    <p>중복·유사 청크는 Max Marginal Relevance(MMR)로 다양화합니다.</p>
  </li>
  <li>
    <p>전체 모아서 재랭킹: 다국어 리랭커 또는 cross-encoder로 상위 50→최종 10개로 줄입니다.</p>
  </li>
</ul>

<ol>
  <li>컨텍스트 빌더</li>
</ol>

<ul>
  <li>
    <p>최종 청크들을 근거 메타데이터와 함께 LLM 컨텍스트로 직조합니다.</p>
  </li>
  <li>
    <p>토큰 한도(max_context_tokens)에 맞춰 자르고, 가능한 한 문단 경계를 유지합니다.</p>
  </li>
  <li>
    <p>규칙을 강제: 답변은 한국어, 인용은 원문 유지, 각 근거에 페이지/섹션 표기.</p>
  </li>
</ul>

<ol>
  <li>운영 팁</li>
</ol>

<ul>
  <li>
    <p>쿼리 라우팅: 분야별 컬렉션이 여러 개면 먼저 라우터(분야 분류기)로 컬렉션을 1~2개로 좁힌 뒤 위 과정을 수행합니다.</p>
  </li>
  <li>
    <p>캐싱: 잦은 쿼리의 임베딩과 상위 결과를 캐시해 지연을 줄입니다.</p>
  </li>
  <li>
    <p>임계값: 유사도 스코어가 임계값 미만이면 “근거 없음”으로 처리해 환각을 줄입니다.</p>
  </li>
  <li>
    <p>권한: 멀티유저라면 문서별 ACL을 메타데이터 필터에 반드시 반영합니다.</p>
  </li>
  <li>
    <p>실험: N, K, 임계값을 검증셋으로 튜닝합니다. 일반적으로 N을 늘리면 리콜↑, 지연/비용↑.</p>
  </li>
</ul>

<p>간단한 의사코드</p>

<ul>
  <li>
    <p>입력: query, filters, top_docs=N, top_chunks_per_doc=K, max_context_tokens</p>
  </li>
  <li>
    <p>출력: ranked_chunks</p>
  </li>
</ul>

<ol>
  <li>
    <p>q_emb = embed(query)</p>
  </li>
  <li>
    <p>doc_candidates = fuse(<br />
vector_search(DOC_INDEX, q_emb, N_vec),<br />
bm25_search(DOC_TEXTS, query, N_bm25)<br />
)[:N]</p>
  </li>
  <li>
    <p>chunk_pool = []<br />
for d in doc_candidates:<br />
chunk_pool += vector_search(d.CHUNK_INDEX, q_emb, K)</p>
  </li>
  <li>
    <p>chunk_pool = mmr_dedup(chunk_pool)</p>
  </li>
  <li>
    <p>ranked = rerank_cross_encoder(query, chunk_pool)</p>
  </li>
  <li>
    <p>context = pack_by_tokens(ranked, max_context_tokens)</p>
  </li>
  <li>
    <p>return context, ranked[:final_k]</p>
  </li>
</ol>

<p>질문으로 주신 두 선택지를 정리하자면</p>

<ul>
  <li>
    <p>모든 RAG 문서를 다 뒤지는 일괄 전수검색은 규모가 커질수록 느려지고 비용이 큽니다.</p>
  </li>
  <li>
    <p>보통은 “가장 관련된 문서 상위 N개”를 추려 그 안에서만 K개 청크를 뽑는 계층적 검색을 씁니다. 100개 문서면 상위 10개 문서 × 문서당 5개 청크 같은 구성이 현실적입니다.</p>
  </li>
</ul>

<p>마지막으로 숫자 레시피 예시</p>

<ul>
  <li>
    <p>top_docs=10, top_chunks_per_doc=5, rerank_top=60→final=12, max_context_tokens=6k</p>
  </li>
  <li>
    <p>하이브리드 비율: vector:bm25 = 0.7:0.3 가중 평균</p>
  </li>
  <li>
    <p>임계값: cosine &lt; 0.25면 제외, 문서 최신가중치 e^(−λ·age_days) 적용</p>
  </li>
</ul>

<p>이렇게 구성하면 정확도·지연·비용의 균형이 좋아지고, 쿼리 폭이 넓어져도 깔끔하게 확장됩니다.</p>

<p class="chat--user">

FunctionTool로 너가 말한 기능을 구현하고 싶어. 대략적인 인터페이스 함수들과 동작을 잡아보자

</p>

<p>좋아, FunctionTool로 RAG 검색을 붙일 때 쓸 “대략적 인터페이스”와 동작 흐름을 잡아볼게. vLLM+gpt-oss에서도 그대로 쓰고, OpenAI Agents SDK에도 거의 그대로 맵핑되도록 설계했어.</p>

<p>핵심 구성요소</p>

<ul>
  <li>
    <p>인덱싱 파이프라인: 파일→텍스트→청크→임베딩→저장</p>
  </li>
  <li>
    <p>검색 파이프라인: 질의→필터→문서 상위 N→청크 상위 K→재랭크→컨텍스트 패킹</p>
  </li>
  <li>
    <p>FunctionTool: 모델이 호출하면 위 검색 파이프라인을 실행해 컨텍스트와 근거 메타를 반환</p>
  </li>
</ul>

<p>데이터 모델(간단)</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ts

type DocMeta = {
  doc_id: string
  title: string
  path?: string
  lang?: 'en'|'ko'|'mixed'
  tags?: string[]
  created_at?: string
  updated_at?: string
  acl?: string[]            // 접근 허용 사용자/역할
}

type Chunk = {
  chunk_id: string
  doc_id: string
  content: string
  page?: number
  section?: string
  tokens?: number
}

type Scored&lt;T&gt; = T &amp; { score: number }
</code></pre></div></div>

<p>FunctionTool 시그니처(검색 전용)<br />
OpenAI-style JSON 스키마를 따라 설계. 툴 이름은 search_docs.</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"search_docs"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"질문과 필터를 받아 관련 문서와 청크를 검색해 LLM 컨텍스트를 만든다."</span><span class="p">,</span><span class="w">
  </span><span class="nl">"parameters"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"object"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"properties"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="nl">"query"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"string"</span><span class="p">,</span><span class="w"> </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"사용자 질문(ko/en 가능)"</span><span class="w"> </span><span class="p">},</span><span class="w">
      </span><span class="nl">"filters"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"object"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"properties"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
          </span><span class="nl">"langs"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"array"</span><span class="p">,</span><span class="w"> </span><span class="nl">"items"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"string"</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="p">},</span><span class="w">
          </span><span class="nl">"tags"</span><span class="p">:</span><span class="w">  </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"array"</span><span class="p">,</span><span class="w"> </span><span class="nl">"items"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"string"</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="p">},</span><span class="w">
          </span><span class="nl">"doc_ids"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"array"</span><span class="p">,</span><span class="w"> </span><span class="nl">"items"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"string"</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="p">},</span><span class="w">
          </span><span class="nl">"updated_after"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"string"</span><span class="p">,</span><span class="w"> </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"ISO8601"</span><span class="w"> </span><span class="p">}</span><span class="w">
        </span><span class="p">}</span><span class="w">
      </span><span class="p">},</span><span class="w">
      </span><span class="nl">"top_docs"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"integer"</span><span class="p">,</span><span class="w"> </span><span class="nl">"default"</span><span class="p">:</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span><span class="w"> </span><span class="nl">"minimum"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="p">},</span><span class="w">
      </span><span class="nl">"top_chunks_per_doc"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"integer"</span><span class="p">,</span><span class="w"> </span><span class="nl">"default"</span><span class="p">:</span><span class="w"> </span><span class="mi">5</span><span class="p">,</span><span class="w"> </span><span class="nl">"minimum"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="p">},</span><span class="w">
      </span><span class="nl">"final_chunks"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"integer"</span><span class="p">,</span><span class="w"> </span><span class="nl">"default"</span><span class="p">:</span><span class="w"> </span><span class="mi">12</span><span class="p">,</span><span class="w"> </span><span class="nl">"minimum"</span><span class="p">:</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="p">},</span><span class="w">
      </span><span class="nl">"max_context_tokens"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"integer"</span><span class="p">,</span><span class="w"> </span><span class="nl">"default"</span><span class="p">:</span><span class="w"> </span><span class="mi">6000</span><span class="p">,</span><span class="w"> </span><span class="nl">"minimum"</span><span class="p">:</span><span class="w"> </span><span class="mi">512</span><span class="w"> </span><span class="p">},</span><span class="w">
      </span><span class="nl">"use_hybrid"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"boolean"</span><span class="p">,</span><span class="w"> </span><span class="nl">"default"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w"> </span><span class="p">},</span><span class="w">
      </span><span class="nl">"use_mmr"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"boolean"</span><span class="p">,</span><span class="w"> </span><span class="nl">"default"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w"> </span><span class="p">},</span><span class="w">
      </span><span class="nl">"use_reranker"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"boolean"</span><span class="p">,</span><span class="w"> </span><span class="nl">"default"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w"> </span><span class="p">},</span><span class="w">
      </span><span class="nl">"translate_query_to_en"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"boolean"</span><span class="p">,</span><span class="w"> </span><span class="nl">"default"</span><span class="p">:</span><span class="w"> </span><span class="kc">false</span><span class="w"> </span><span class="p">}</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="nl">"required"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"query"</span><span class="p">]</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>FunctionTool 반환 포맷<br />
LLM이 바로 먹일 수 있게 컨텍스트 텍스트와 근거 리스트를 함께 줌.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ts

type SearchResult = {
  context: string                 // LLM에 바로 넣을 컨텍스트
  citations: Array&lt;{
    doc_id: string
    title?: string
    page?: number
    section?: string
    snippet: string
    score: number
  }&gt;
  diagnostics?: {
    used_docs: Array&lt;Scored&lt;DocMeta&gt;&gt;
    used_chunks: Array&lt;Scored&lt;Chunk&gt;&gt;
    query_lang?: 'ko'|'en'|'unknown'
    translated_query?: string
  }
}
</code></pre></div></div>

<p>동작 의사코드</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">search_docs</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">top_docs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">top_chunks_per_doc</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                <span class="n">final_chunks</span><span class="o">=</span><span class="mi">12</span><span class="p">,</span> <span class="n">max_context_tokens</span><span class="o">=</span><span class="mi">6000</span><span class="p">,</span>
                <span class="n">use_hybrid</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">use_mmr</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">use_reranker</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
                <span class="n">translate_query_to_en</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">SearchResult</span><span class="p">:</span>

    <span class="c1"># 0) 권한/필터링 프레딕트
</span>    <span class="n">candidate_docs</span> <span class="o">=</span> <span class="nf">filter_docs</span><span class="p">(</span><span class="n">filters</span><span class="p">)</span>  <span class="c1"># lang/tags/doc_ids/updated_after/ACL 반영
</span>
    <span class="c1"># 1) 쿼리 준비
</span>    <span class="n">lang</span> <span class="o">=</span> <span class="nf">detect_lang</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>  <span class="c1"># ko/en 추정
</span>    <span class="n">q_for_vec</span> <span class="o">=</span> <span class="n">query</span>
    <span class="k">if</span> <span class="n">translate_query_to_en</span> <span class="ow">and</span> <span class="n">lang</span> <span class="o">==</span> <span class="sh">'</span><span class="s">ko</span><span class="sh">'</span><span class="p">:</span>
        <span class="n">q_for_vec</span> <span class="o">=</span> <span class="nf">translate_ko_to_en</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>  <span class="c1"># 선택 기능
</span>
    <span class="c1"># 2) 문서 레벨 프리셀렉션
</span>    <span class="n">doc_scores_vec</span> <span class="o">=</span> <span class="nf">vector_search_docs</span><span class="p">(</span><span class="n">candidate_docs</span><span class="p">,</span> <span class="n">q_for_vec</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="n">top_docs</span><span class="o">*</span><span class="mi">3</span><span class="p">)</span>
    <span class="n">doc_scores_kw</span>  <span class="o">=</span> <span class="nf">bm25_search_docs</span><span class="p">(</span><span class="n">candidate_docs</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="n">top_docs</span><span class="o">*</span><span class="mi">3</span><span class="p">)</span> <span class="k">if</span> <span class="n">use_hybrid</span> <span class="k">else</span> <span class="p">[]</span>
    <span class="n">doc_candidates</span> <span class="o">=</span> <span class="nf">fuse_scores</span><span class="p">(</span><span class="n">doc_scores_vec</span><span class="p">,</span> <span class="n">doc_scores_kw</span><span class="p">)[:</span><span class="n">top_docs</span><span class="p">]</span>

    <span class="c1"># 3) 청크 레벨 정밀 검색
</span>    <span class="n">chunk_pool</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">doc_candidates</span><span class="p">:</span>
        <span class="n">chunks_vec</span> <span class="o">=</span> <span class="nf">vector_search_chunks</span><span class="p">(</span><span class="n">doc</span><span class="p">.</span><span class="n">doc_id</span><span class="p">,</span> <span class="n">q_for_vec</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="n">top_chunks_per_doc</span><span class="o">*</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">chunks_kw</span>  <span class="o">=</span> <span class="nf">bm25_search_chunks</span><span class="p">(</span><span class="n">doc</span><span class="p">.</span><span class="n">doc_id</span><span class="p">,</span> <span class="n">query</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="n">top_chunks_per_doc</span><span class="o">*</span><span class="mi">2</span><span class="p">)</span> <span class="k">if</span> <span class="n">use_hybrid</span> <span class="k">else</span> <span class="p">[]</span>
        <span class="n">chunk_pool</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="nf">fuse_scores</span><span class="p">(</span><span class="n">chunks_vec</span><span class="p">,</span> <span class="n">chunks_kw</span><span class="p">))</span>

    <span class="k">if</span> <span class="n">use_mmr</span><span class="p">:</span>
        <span class="n">chunk_pool</span> <span class="o">=</span> <span class="nf">mmr_diversify</span><span class="p">(</span><span class="n">chunk_pool</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">c</span><span class="p">:</span> <span class="n">c</span><span class="p">.</span><span class="n">content</span><span class="p">,</span> <span class="n">top</span><span class="o">=</span><span class="n">final_chunks</span><span class="o">*</span><span class="mi">4</span><span class="p">)</span>

    <span class="c1"># 4) 재랭킹
</span>    <span class="n">ranked</span> <span class="o">=</span> <span class="n">chunk_pool</span>
    <span class="k">if</span> <span class="n">use_reranker</span><span class="p">:</span>
        <span class="n">ranked</span> <span class="o">=</span> <span class="nf">cross_encoder_rerank</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">chunk_pool</span><span class="p">)</span>  <span class="c1"># 상위 50→정렬
</span>
    <span class="c1"># 5) 컨텍스트 패킹
</span>    <span class="n">selected</span> <span class="o">=</span> <span class="nf">take_until_token_budget</span><span class="p">(</span><span class="n">ranked</span><span class="p">,</span> <span class="n">final_chunks</span><span class="p">,</span> <span class="n">max_context_tokens</span><span class="p">)</span>
    <span class="n">context</span> <span class="o">=</span> <span class="nf">build_context</span><span class="p">(</span><span class="n">selected</span><span class="p">)</span>  <span class="c1"># [Doc:Title p.X] + 본문… 형태
</span>
    <span class="c1"># 6) 인용/디버그 정보
</span>    <span class="n">citations</span> <span class="o">=</span> <span class="p">[{</span>
        <span class="sh">"</span><span class="s">doc_id</span><span class="sh">"</span><span class="p">:</span> <span class="n">c</span><span class="p">.</span><span class="n">doc_id</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">title</span><span class="sh">"</span><span class="p">:</span> <span class="nf">get_title</span><span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">doc_id</span><span class="p">),</span>
        <span class="sh">"</span><span class="s">page</span><span class="sh">"</span><span class="p">:</span> <span class="n">c</span><span class="p">.</span><span class="n">page</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">section</span><span class="sh">"</span><span class="p">:</span> <span class="n">c</span><span class="p">.</span><span class="n">section</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">snippet</span><span class="sh">"</span><span class="p">:</span> <span class="nf">shorten</span><span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">content</span><span class="p">,</span> <span class="mi">300</span><span class="p">),</span>
        <span class="sh">"</span><span class="s">score</span><span class="sh">"</span><span class="p">:</span> <span class="nf">float</span><span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">score</span><span class="p">)</span>
    <span class="p">}</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">selected</span><span class="p">]</span>

    <span class="n">diags</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">used_docs</span><span class="sh">"</span><span class="p">:</span> <span class="n">doc_candidates</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">used_chunks</span><span class="sh">"</span><span class="p">:</span> <span class="n">selected</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">query_lang</span><span class="sh">"</span><span class="p">:</span> <span class="n">lang</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">translated_query</span><span class="sh">"</span><span class="p">:</span> <span class="n">q_for_vec</span> <span class="k">if</span> <span class="n">q_for_vec</span> <span class="o">!=</span> <span class="n">query</span> <span class="k">else</span> <span class="bp">None</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="p">{</span><span class="sh">"</span><span class="s">context</span><span class="sh">"</span><span class="p">:</span> <span class="n">context</span><span class="p">,</span> <span class="sh">"</span><span class="s">citations</span><span class="sh">"</span><span class="p">:</span> <span class="n">citations</span><span class="p">,</span> <span class="sh">"</span><span class="s">diagnostics</span><span class="sh">"</span><span class="p">:</span> <span class="n">diags</span><span class="p">}</span>
</code></pre></div></div>

<p>컨텍스트 빌드 규칙(요약)</p>

<ul>
  <li>
    <p>문서/페이지/섹션 메타를 명시</p>
  </li>
  <li>
    <p>청크 사이에 구분자</p>
  </li>
  <li>
    <p>원문이 영어라도 그대로 두고, 최종 답변 언어는 LLM 시스템 프롬프트로 강제</p>
  </li>
</ul>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Source] Title="Spec ABC", DocID=doc_123, Page=5, Section="3.2"
"Chunk content here..."

[Source] Title="Spec XYZ", DocID=doc_987, Page=12
"Another chunk..."
</code></pre></div></div>

<p>Agents SDK에 등록 예시</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">openai</span> <span class="kn">import</span> <span class="n">OpenAI</span>
<span class="n">client</span> <span class="o">=</span> <span class="nc">OpenAI</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">search_docs_tool</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
    <span class="k">return</span> <span class="nf">search_docs</span><span class="p">(</span><span class="o">**</span><span class="n">params</span><span class="p">)</span>

<span class="n">tools</span> <span class="o">=</span> <span class="p">[{</span>
  <span class="sh">"</span><span class="s">type</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">function</span><span class="sh">"</span><span class="p">,</span>
  <span class="sh">"</span><span class="s">function</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span>
    <span class="sh">"</span><span class="s">name</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">search_docs</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">description</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s">문서/청크 검색</span><span class="sh">"</span><span class="p">,</span>
    <span class="sh">"</span><span class="s">parameters</span><span class="sh">"</span><span class="p">:</span> <span class="p">{</span> <span class="p">...</span> <span class="n">위</span> <span class="n">JSON</span> <span class="n">스키마</span> <span class="p">...</span> <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}]</span>

<span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
  <span class="p">{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span><span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span><span class="sh">"</span><span class="s">답변은 한국어로. 반드시 search_docs를 이용해 근거를 확인하고, 인용을 포함해라.</span><span class="sh">"</span><span class="p">},</span>
  <span class="p">{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span><span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span><span class="sh">"</span><span class="s">영어로 된 제품 보증 조건 요약해줘</span><span class="sh">"</span><span class="p">}</span>
<span class="p">]</span>

<span class="n">resp</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="n">chat</span><span class="p">.</span><span class="n">completions</span><span class="p">.</span><span class="nf">create</span><span class="p">(</span>
  <span class="n">model</span><span class="o">=</span><span class="sh">"</span><span class="s">gpt-4o-mini</span><span class="sh">"</span><span class="p">,</span>
  <span class="n">messages</span><span class="o">=</span><span class="n">messages</span><span class="p">,</span>
  <span class="n">tools</span><span class="o">=</span><span class="n">tools</span>
<span class="p">)</span>
<span class="c1"># tool_calls 감지 → search_docs_tool 실행 → 결과를 tool 메시지로 첨부 → 모델 재호출
</span></code></pre></div></div>

<p>vLLM+gpt-oss 커스텀 루프</p>

<ul>
  <li>
    <p>vLLM은 툴 호출 오케스트레이션이 없으니, 모델 출력의 function_call(JSON)을 직접 파싱해서 search_docs를 호출한 뒤 결과를 다시 모델에 넣는 루프를 구현하면 된다.</p>
  </li>
  <li>
    <p>또는 “수동 모드”로, 사용자 질문이 오면 서버에서 search_docs를 먼저 실행해 context를 만들고, 그걸 user 메시지에 붙여 모델을 한 번만 호출하는 방식도 간단하고 튼튼하다.</p>
  </li>
</ul>

<p>인덱싱용 보조 툴(선택)<br />
파일 업로드/재빌드/상태 확인을 위해 추가 FunctionTool을 두면 운영이 편하다.</p>

<ul>
  <li>
    <p>upsert_documents(files, tags, lang_hint)</p>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>rebuild_index(scope='all'</td>
          <td>'doc_id')</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>describe_doc(doc_id)</p>
  </li>
  <li>purge_docs(doc_ids)</li>
</ul>

<p>최소 운영 파라미터의 추천 기본값</p>

<ul>
  <li>
    <p>top_docs=10</p>
  </li>
  <li>
    <p>top_chunks_per_doc=5</p>
  </li>
  <li>
    <p>final_chunks=12</p>
  </li>
  <li>
    <p>max_context_tokens=6000</p>
  </li>
  <li>
    <p>use_hybrid=true, use_mmr=true, use_reranker=true</p>
  </li>
  <li>
    <p>translate_query_to_en=false (다국어 임베딩이면 불필요, 영어 전용 임베딩이면 true)</p>
  </li>
</ul>

<p>여기까지가 견고한 골격이야. 다음 단계로는 너희가 쓰는 임베딩/벡터DB를 가정해서 search_docs 내부의 vector_search_*와 bm25, reranker, 패킹 함수를 실코드로 메워 넣으면 바로 붙일 수 있다.</p>

<p class="chat--user">

Database를 만들기도 해야해. 이를 위해 문서를 DB에 추가하거나 하는 것도 필요할 것 같은데

</p>

<p>좋아, RAG용 Database까지 포함한 “문서 수집·저장·인덱싱·검색” 풀사이클 인터페이스를 잡아보자. 운영을 염두에 두고 설계하면 나중에 안 아파요.</p>

<p>핵심 구조 개요</p>

<ol>
  <li>
    <p>원본 저장소: 업로드한 파일을 보관하는 Object Storage(S3, GCS, MinIO 등)</p>
  </li>
  <li>
    <p>메타·텍스트 DB: 문서/청크/태그/ACL/상태를 관리하는 RDB(PostgreSQL 권장)</p>
  </li>
  <li>
    <p>벡터 인덱스: pgvector 또는 외부 벡터DB(FAISS, Milvus, Chroma, Qdrant)</p>
  </li>
  <li>
    <p>잡 큐: 임베딩·재인덱싱 같은 비동기 작업용(예: Celery, RQ, Sidekiq, Temporal)</p>
  </li>
</ol>

<p>권장 구현 프로파일<br />
PostgreSQL + pgvector 한 방으로 끝내기<br />
장점: 트랜잭션, 조인, 필터링, 권한체크가 깔끔. 운영 단순.<br />
대안: 텍스트는 Postgres, 벡터는 FAISS/Milvus로 분리. 성능 튜닝 자유도↑.</p>

<p>데이터 모델 스키마(Postgres + pgvector)<br />
SQL DDL 예시. 임베딩 차원은 예로 1024를 사용.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sql

create extension if not exists vector;

create table documents (
  doc_id          uuid primary key,
  title           text not null,
  source_uri      text,                 -- S3 키, GCS 경로 등
  mime_type       text,
  lang            text,                 -- 'en','ko','mixed' 등
  tags            text[],
  bytes           bigint,
  pages           int,
  created_at      timestamptz default now(),
  updated_at      timestamptz default now(),
  status          text not null default 'ready',  -- 'ready','processing','error','disabled'
  acl_roles       text[] default array[]::text[], -- 접근 허용 역할
  acl_users       text[] default array[]::text[]  -- 접근 허용 사용자
);

create table chunks (
  chunk_id        uuid primary key,
  doc_id          uuid references documents(doc_id) on delete cascade,
  page            int,
  section         text,
  content         text not null,
  token_count     int,
  created_at      timestamptz default now(),
  updated_at      timestamptz default now()
);

-- 하나의 컬럼에 다국어 임베딩을 저장(권장). 필요하면 영어 전용 추가 컬럼도.
create table chunk_embeddings (
  chunk_id        uuid primary key references chunks(chunk_id) on delete cascade,
  emb_mtext       vector(1024),         -- 다국어 임베딩
  emb_en          vector(1024)          -- 선택: 영어 전용 임베딩
);

-- BM25 대체용 전체검색 인덱스
create index idx_chunks_fts on chunks using gin (to_tsvector('simple', content));

-- 태그/언어/상태/업데이트일 필터링
create index idx_documents_tags on documents using gin (tags);
create index idx_documents_lang on documents (lang);
create index idx_documents_status on documents (status);
create index idx_documents_updated on documents (updated_at);

-- 벡터 인덱스(IVFFlat 예시). 규모 커지면 lists 파라미터 튜닝.
create index idx_chunk_emb_mtext on chunk_embeddings using ivfflat (emb_mtext vector_cosine_ops) with (lists = 200);
create index idx_chunk_emb_en    on chunk_embeddings using ivfflat (emb_en    vector_cosine_ops) with (lists = 200);
</code></pre></div></div>

<p>스토리지 계층<br />
원문 파일은 RDB에 넣지 말고 Object Storage에 넣고 documents.source_uri로 연결. 썸네일·텍스트 추출 캐시를 두면 속도가 빨라짐.</p>

<p>FunctionTool 인터페이스 설계<br />
검색뿐 아니라 업서트·재인덱싱·관리까지 FunctionTool로 노출하면 LLM 에이전트나 관리용 UI에서 자동화하기 좋다.</p>

<ol>
  <li>문서 업서트<br />
이 함수는 DB와 스토리지를 갱신하고, 비동기 인덱싱 잡을 발행.</li>
</ol>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"upsert_documents"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"문서를 DB와 스토리지에 추가/갱신하고 인덱싱 잡을 큐에 넣는다."</span><span class="p">,</span><span class="w">
  </span><span class="nl">"parameters"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"object"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"properties"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="nl">"items"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"array"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"items"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
          </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"object"</span><span class="p">,</span><span class="w">
          </span><span class="nl">"properties"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
            </span><span class="nl">"title"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"string"</span><span class="w"> </span><span class="p">},</span><span class="w">
            </span><span class="nl">"source_uri"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"string"</span><span class="w"> </span><span class="p">},</span><span class="w">
            </span><span class="nl">"mime_type"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"string"</span><span class="w"> </span><span class="p">},</span><span class="w">
            </span><span class="nl">"lang_hint"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"string"</span><span class="w"> </span><span class="p">},</span><span class="w">
            </span><span class="nl">"tags"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"array"</span><span class="p">,</span><span class="w"> </span><span class="nl">"items"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"string"</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="p">},</span><span class="w">
            </span><span class="nl">"acl_roles"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"array"</span><span class="p">,</span><span class="w"> </span><span class="nl">"items"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"string"</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="p">},</span><span class="w">
            </span><span class="nl">"acl_users"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"array"</span><span class="p">,</span><span class="w"> </span><span class="nl">"items"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"string"</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="p">},</span><span class="w">
            </span><span class="nl">"doc_id"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"string"</span><span class="p">,</span><span class="w"> </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"갱신 시 지정"</span><span class="w"> </span><span class="p">}</span><span class="w">
          </span><span class="p">},</span><span class="w">
          </span><span class="nl">"required"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"title"</span><span class="p">,</span><span class="w"> </span><span class="s2">"source_uri"</span><span class="p">]</span><span class="w">
        </span><span class="p">}</span><span class="w">
      </span><span class="p">},</span><span class="w">
      </span><span class="nl">"reindex"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"boolean"</span><span class="p">,</span><span class="w"> </span><span class="nl">"default"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w"> </span><span class="p">}</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="nl">"required"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"items"</span><span class="p">]</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>동작</p>

<ol>
  <li>
    <p>source_uri 유효성 확인 및 메타 추출(bytes, pages 등)</p>
  </li>
  <li>
    <p>documents upsert</p>
  </li>
  <li>
    <p>상태를 processing으로 바꾸고 인덱싱 잡 enqueue</p>
  </li>
  <li>
    <p>인덱싱 워커(비동기 잡)<br />
추출→청크→임베딩→저장 파이프라인. 실패 시 documents.status='error'와 메시지 저장.<br />
의사코드</p>
  </li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">worker_index_document</span><span class="p">(</span><span class="n">doc_id</span><span class="p">):</span>
    <span class="n">doc</span> <span class="o">=</span> <span class="n">db</span><span class="p">.</span><span class="nf">get_document</span><span class="p">(</span><span class="n">doc_id</span><span class="p">)</span>
    <span class="n">raw</span> <span class="o">=</span> <span class="n">storage</span><span class="p">.</span><span class="nf">download</span><span class="p">(</span><span class="n">doc</span><span class="p">.</span><span class="n">source_uri</span><span class="p">)</span>
    <span class="n">text</span><span class="p">,</span> <span class="n">pages</span> <span class="o">=</span> <span class="nf">extract_text</span><span class="p">(</span><span class="n">raw</span><span class="p">)</span>             <span class="c1"># pypdf/unstructured/docling
</span>    <span class="n">chunks</span> <span class="o">=</span> <span class="nf">chunking</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>                     <span class="c1"># 200~400 토큰 권장
</span>    <span class="n">db</span><span class="p">.</span><span class="nf">delete_chunks</span><span class="p">(</span><span class="n">doc_id</span><span class="p">)</span>                    <span class="c1"># 전체 재빌드 시
</span>    <span class="k">for</span> <span class="n">ch</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">:</span>
        <span class="n">chunk_id</span> <span class="o">=</span> <span class="nf">uuid4</span><span class="p">()</span>
        <span class="n">db</span><span class="p">.</span><span class="nf">insert_chunk</span><span class="p">(</span><span class="n">chunk_id</span><span class="p">,</span> <span class="n">doc_id</span><span class="p">,</span> <span class="n">ch</span><span class="p">.</span><span class="n">page</span><span class="p">,</span> <span class="n">ch</span><span class="p">.</span><span class="n">section</span><span class="p">,</span> <span class="n">ch</span><span class="p">.</span><span class="n">content</span><span class="p">,</span> <span class="n">ch</span><span class="p">.</span><span class="n">tokens</span><span class="p">)</span>
    <span class="c1"># 임베딩
</span>    <span class="n">vecs_m</span> <span class="o">=</span> <span class="nf">embed_multilingual</span><span class="p">([</span><span class="n">c</span><span class="p">.</span><span class="n">content</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">])</span>
    <span class="n">vecs_en</span> <span class="o">=</span> <span class="bp">None</span>  <span class="c1"># 필요하면 번역 후 또는 영어전용 임베딩
</span>    <span class="n">db</span><span class="p">.</span><span class="nf">insert_embeddings</span><span class="p">([</span> <span class="p">(</span><span class="n">chunks</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="nb">id</span><span class="p">,</span> <span class="n">vecs_m</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">vecs_en</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">if</span> <span class="n">vecs_en</span> <span class="k">else</span> <span class="bp">None</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">chunks</span><span class="p">))</span> <span class="p">])</span>
    <span class="n">db</span><span class="p">.</span><span class="nf">update_document_status</span><span class="p">(</span><span class="n">doc_id</span><span class="p">,</span> <span class="sh">'</span><span class="s">ready</span><span class="sh">'</span><span class="p">)</span>
</code></pre></div></div>

<ol>
  <li>문서 상태 조회/설명</li>
</ol>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"describe_doc"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"문서 메타데이터와 인덱싱 상태를 조회한다."</span><span class="p">,</span><span class="w">
  </span><span class="nl">"parameters"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"object"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"properties"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"doc_id"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"string"</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="p">},</span><span class="w">
    </span><span class="nl">"required"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"doc_id"</span><span class="p">]</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>반환은 title, lang, tags, updated_at, status, chunk_count 등.</p>

<ol>
  <li>선택적 재빌드</li>
</ol>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"rebuild_index"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"문서 또는 전체 컬렉션의 인덱스를 재생성한다."</span><span class="p">,</span><span class="w">
  </span><span class="nl">"parameters"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"object"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"properties"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="nl">"scope"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"string"</span><span class="p">,</span><span class="w"> </span><span class="nl">"enum"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"all"</span><span class="p">,</span><span class="s2">"doc"</span><span class="p">],</span><span class="w"> </span><span class="nl">"default"</span><span class="p">:</span><span class="w"> </span><span class="s2">"doc"</span><span class="w"> </span><span class="p">},</span><span class="w">
      </span><span class="nl">"doc_id"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"string"</span><span class="w"> </span><span class="p">}</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="nl">"required"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"scope"</span><span class="p">]</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<ol>
  <li>삭제</li>
</ol>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"purge_docs"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"description"</span><span class="p">:</span><span class="w"> </span><span class="s2">"문서를 완전히 삭제한다(청크, 임베딩 포함)."</span><span class="p">,</span><span class="w">
  </span><span class="nl">"parameters"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"object"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"properties"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"doc_ids"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"array"</span><span class="p">,</span><span class="w"> </span><span class="nl">"items"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"string"</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="p">}</span><span class="w"> </span><span class="p">},</span><span class="w">
    </span><span class="nl">"required"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"doc_ids"</span><span class="p">]</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<ol>
  <li>검색 툴(앞서 정의한 search_docs)<br />
필터에 tags/lang/updated_after/acl 포함. top_docs, top_chunks_per_doc, final_chunks로 폭을 제어.</li>
</ol>

<p>권한(ACL) 처리<br />
검색 쿼리에서 반드시 다음을 적용.</p>

<ol>
  <li>
    <p>로그인 유저의 roles, user_id를 받아 documents.acl_roles/acl_users에 매칭</p>
  </li>
  <li>
    <p>필터 단계에서 미리 제외</p>
  </li>
  <li>
    <p>감사 로그: 누가 어떤 doc_id를 근거로 보았는지 기록</p>
  </li>
</ol>

<p>성능 팁</p>

<ol>
  <li>
    <p>토큰 예산 우선: final_chunks는 8~16 사이, 총 4~8k 토큰 내에서 패킹</p>
  </li>
  <li>
    <p>인덱스 튜닝: IVFFlat lists, probes 조정. pgvector의 SET ivfflat.probes=값</p>
  </li>
  <li>
    <p>하이브리드: to_tsvector(simple)로 키워드 인덱스. 고유명사/숫자 강화</p>
  </li>
  <li>
    <p>캐싱: 쿼리→상위 doc_ids, chunk_ids 캐시. 임베딩 결과도 메모리 캐시</p>
  </li>
  <li>
    <p>최신 가중치: score = α·cosine + β·bm25 + γ·recency_boost</p>
  </li>
</ol>

<p>간단 API 흐름 예시</p>

<ol>
  <li>
    <p>클라이언트가 파일 업로드 → 백엔드가 S3에 저장 후 upsert_documents 호출</p>
  </li>
  <li>
    <p>워커가 인덱싱 완료 → 문서 status=ready</p>
  </li>
  <li>
    <p>사용자가 질문 → search_docs 호출 → 컨텍스트와 citations 반환</p>
  </li>
  <li>
    <p>LLM 호출 시 컨텍스트 삽입 → 한국어 답변 생성</p>
  </li>
</ol>

<p>서버 사이드 의사코드 스니펫<br />
문서 업서트</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">upsert_documents</span><span class="p">(</span><span class="n">items</span><span class="p">,</span> <span class="n">reindex</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">it</span> <span class="ow">in</span> <span class="n">items</span><span class="p">:</span>
        <span class="n">doc_id</span> <span class="o">=</span> <span class="n">it</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">doc_id</span><span class="sh">"</span><span class="p">)</span> <span class="ow">or</span> <span class="nf">uuid4</span><span class="p">()</span>
        <span class="n">db</span><span class="p">.</span><span class="nf">upsert_document</span><span class="p">(</span>
            <span class="n">doc_id</span><span class="o">=</span><span class="n">doc_id</span><span class="p">,</span>
            <span class="n">title</span><span class="o">=</span><span class="n">it</span><span class="p">[</span><span class="sh">"</span><span class="s">title</span><span class="sh">"</span><span class="p">],</span>
            <span class="n">source_uri</span><span class="o">=</span><span class="n">it</span><span class="p">[</span><span class="sh">"</span><span class="s">source_uri</span><span class="sh">"</span><span class="p">],</span>
            <span class="n">mime_type</span><span class="o">=</span><span class="n">it</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">mime_type</span><span class="sh">"</span><span class="p">),</span>
            <span class="n">lang</span><span class="o">=</span><span class="n">it</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">lang_hint</span><span class="sh">"</span><span class="p">),</span>
            <span class="n">tags</span><span class="o">=</span><span class="n">it</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">tags</span><span class="sh">"</span><span class="p">,</span> <span class="p">[]),</span>
            <span class="n">acl_roles</span><span class="o">=</span><span class="n">it</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">acl_roles</span><span class="sh">"</span><span class="p">,</span> <span class="p">[]),</span>
            <span class="n">acl_users</span><span class="o">=</span><span class="n">it</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="sh">"</span><span class="s">acl_users</span><span class="sh">"</span><span class="p">,</span> <span class="p">[]),</span>
            <span class="n">status</span><span class="o">=</span><span class="sh">"</span><span class="s">processing</span><span class="sh">"</span> <span class="k">if</span> <span class="n">reindex</span> <span class="k">else</span> <span class="sh">"</span><span class="s">ready</span><span class="sh">"</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">reindex</span><span class="p">:</span>
            <span class="n">queue</span><span class="p">.</span><span class="nf">enqueue</span><span class="p">(</span><span class="sh">"</span><span class="s">worker_index_document</span><span class="sh">"</span><span class="p">,</span> <span class="n">doc_id</span><span class="o">=</span><span class="n">doc_id</span><span class="p">)</span>
        <span class="n">results</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">doc_id</span><span class="sh">"</span><span class="p">:</span> <span class="nf">str</span><span class="p">(</span><span class="n">doc_id</span><span class="p">),</span> <span class="sh">"</span><span class="s">queued</span><span class="sh">"</span><span class="p">:</span> <span class="n">reindex</span><span class="p">})</span>
    <span class="k">return</span> <span class="p">{</span><span class="sh">"</span><span class="s">items</span><span class="sh">"</span><span class="p">:</span> <span class="n">results</span><span class="p">}</span>
</code></pre></div></div>

<p>검색 핵심 쿼리(pgvector)</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sql

-- 다국어 임베딩으로 top N 청크
select c.chunk_id, c.doc_id, c.page, c.section, c.content,
       1 - (ce.emb_mtext &lt;=&gt; :qvec) as score
from chunk_embeddings ce
join chunks c on c.chunk_id = ce.chunk_id
join documents d on d.doc_id = c.doc_id
where d.status = 'ready'
  and (:lang is null or d.lang = :lang)
  and (:tags is null or d.tags &amp;&amp; :tags::text[])
  and (:user_id = any(d.acl_users) or array_length(d.acl_users,1) is null
       or :role &amp;&amp; d.acl_roles)
order by ce.emb_mtext &lt;=&gt; :qvec
limit :k;
</code></pre></div></div>

<p>운영 체크리스트<br />
문서 상태 전이: ready ↔ processing ↔ error<br />
중복 방지: 같은 source_uri+hash로 idempotent 업서트<br />
관찰성: 인덱싱 시간, 청크 수, 임베딩 시간, 검색 지연, 히트율 지표 수집<br />
재현성: 임베딩 모델 버전 저장. 교체 시 롤링 재인덱싱</p>

<p>정리</p>

<ol>
  <li>
    <p>문서 메타·청크·임베딩을 RDB로 명확히 분리하고, 파일은 Object Storage에 둔다.</p>
  </li>
  <li>
    <p>FunctionTool로 upsert/rebuild/describe/purge/search를 노출해 LLM이 필요한 순간에만 호출하게 한다.</p>
  </li>
  <li>
    <p>비동기 워커를 두어 인덱싱을 오프로드한다.</p>
  </li>
  <li>
    <p>pgvector 하나로도 충분히 단단한 RAG DB를 만들 수 있다.</p>
  </li>
</ol>

<p>다음으로 원하시면 이 스키마를 기준으로 실제 Python 저장소 레이어(ORM), 워커(Celery), 그리고 search_docs 내부 SQL까지 꽂아 넣은 미니 레퍼런스를 만들어줄게.</p>

<p class="chat--user">

임베딩 등은 그냥 유저의 로컬 파일 시스템에 저장한다고 가정하자.

</p>

<p>좋아. “임베딩과 인덱스까지 모두 로컬 파일 시스템에 저장”한다는 가정으로, 풀사이클 인터페이스와 디렉터리 구조, 파일 포맷, FunctionTool 시그니처, 동작 스케치를 잡아줄게. 외부 DB 없이도 단단히 돌아가도록 설계했어. 필요 라이브러리는 pypdf/unstructured, sentence-transformers(or bge/e5), faiss, sqlite3 정도로 가정한다.</p>

<p>설계 요약</p>

<ol>
  <li>
    <p>스토리지 계층은 전부 로컬 디렉터리. 원본 파일과 파생 산출물(텍스트, 청크, 임베딩, 인덱스)을 규칙적으로 저장.</p>
  </li>
  <li>
    <p>메타데이터는 SQLite 한 파일로 관리해 조회·필터·ACL가 빠름. 대안으로 JSONL만 써도 되지만 규모가 커지면 느려짐.</p>
  </li>
  <li>
    <p>벡터 검색은 FAISS를 사용하고 인덱스 파일로 영속화. 키워드 검색은 SQLite FTS5로 대체.</p>
  </li>
  <li>
    <p>FunctionTool은 업서트, 재인덱싱, 검색, 설명, 삭제까지 한 세트로 노출.</p>
  </li>
</ol>

<p>디렉터리 구조 예시<br />
rag_store/<br />
blobs/ 원본 파일 보관<br />
{doc_id}/{filename}<br />
texts/ 텍스트 추출 캐시<br />
{doc_id}.txt<br />
chunks/ 청크 JSONL<br />
{doc_id}.jsonl [{"chunk_id","page","section","content","tokens"}…]<br />
embeddings/ 임베딩 파일<br />
mtext/{doc_id}.npy float32 [num_chunks, dim]<br />
en/{doc_id}.npy 선택: 영어 전용<br />
indices/ 전체 컬렉션용 인덱스<br />
faiss_mtext.index<br />
faiss_en.index<br />
manifest/ 버전·모델 추적<br />
embedding_manifest.json [{"model":"bge-m3","dim":1024,"built_at":…}]<br />
locks/<br />
index.lock<br />
rag_meta.sqlite SQLite 메타DB(문서·청크·ACL·FTS5)</p>

<p>SQLite 스키마 요약<br />
documents(doc_id pk, title, source_path, mime, lang, tags json, bytes, pages, status, acl_users json, acl_roles json, created_at, updated_at, embed_model_mtext, embed_model_en)<br />
chunks(chunk_id pk, doc_id fk, page, section, content, tokens)<br />
fts_chunks(content) USING FTS5(content, content='chunks', content_rowid='rowid')<br />
인덱스: documents(status, updated_at), chunks(doc_id)</p>

<p>FunctionTool 인터페이스</p>

<ol>
  <li>
    <p>upsert_documents<br />
name: upsert_documents<br />
description: 로컬 파일을 등록하고 인덱싱 잡을 트리거한다.<br />
parameters:<br />
items: [{title, source_path, lang_hint?, tags?, acl_users?, acl_roles?, doc_id?}]<br />
reindex: boolean = true</p>
  </li>
  <li>
    <p>rebuild_index<br />
name: rebuild_index<br />
description: 전체 또는 특정 문서의 임베딩·FAISS 인덱스를 재생성한다.<br />
parameters:<br />
scope: "all"|"doc"<br />
doc_id?: string<br />
which: "mtext"|"en"|"both" = "mtext"</p>
  </li>
  <li>
    <p>describe_doc<br />
name: describe_doc<br />
description: 문서 메타·상태·청크 수를 조회한다.<br />
parameters:<br />
doc_id: string</p>
  </li>
  <li>
    <p>purge_docs<br />
name: purge_docs<br />
description: 문서를 로컬 스토리지와 메타DB, 인덱스에서 완전 삭제한다.<br />
parameters:<br />
doc_ids: string[]</p>
  </li>
  <li>
    <p>search_docs<br />
name: search_docs<br />
description: 쿼리와 필터로 문서 상위 N → 청크 상위 K → 재랭크 후 컨텍스트 빌드<br />
parameters:<br />
query: string<br />
filters?: { langs?: string[], tags?: string[], doc_ids?: string[], updated_after?: string }<br />
top_docs?: int = 10<br />
top_chunks_per_doc?: int = 5<br />
final_chunks?: int = 12<br />
max_context_tokens?: int = 6000<br />
use_hybrid?: boolean = true 벡터 + FTS5<br />
use_mmr?: boolean = true<br />
use_reranker?: boolean = true 크로스 인코더가 있으면 사용<br />
translate_query_to_en?: boolean = false</p>
  </li>
</ol>

<p>주요 동작 스케치</p>

<p>A) 업서트와 인덱싱</p>

<ol>
  <li>
    <p>upsert_documents(items):<br />
a. 파일 존재 확인 → rag_store/blobs/{doc_id}/에 복사<br />
b. rag_meta.sqlite의 documents에 upsert, status='processing'<br />
c. 인덱싱 워커 트리거(worker_index_document(doc_id))</p>
  </li>
  <li>
    <p>worker_index_document(doc_id):<br />
a. 텍스트 추출 → rag_store/texts/{doc_id}.txt<br />
b. 청크 생성 → rag_store/chunks/{doc_id}.jsonl 저장, chunks 테이블 교체 삽입<br />
c. 임베딩 생성 → rag_store/embeddings/mtext/{doc_id}.npy 저장<br />
d. SQLite FTS5 동기화: chunks를 fts_chunks에 삽입<br />
e. documents.status='ready', updated_at=now<br />
f. 글로벌 FAISS 인덱스 갱신: indices/faiss_mtext.index 로드 → add_with_ids → 저장</p>
  </li>
</ol>

<p>B) 검색<br />
search_docs(query, …):</p>

<ol>
  <li>
    <p>쿼리 전처리와 임베딩 준비(필요시 ko→en 번역)</p>
  </li>
  <li>
    <p>문서 레벨 프리셀렉션<br />
a. FTS5 문서 후보: SELECT doc_id FROM fts_chunks WHERE fts_chunks MATCH ? LIMIT N_bm25<br />
b. FAISS 후보: 쿼리 임베딩으로 topN 청크 검색 → doc_id 집계 상위 N_vec<br />
c. 두 결과를 RRF로 합쳐 상위 top_docs 도출</p>
  </li>
  <li>
    <p>청크 레벨 정밀 검색<br />
a. 선택 문서별로 top_chunks_per_doc씩 FAISS 검색<br />
b. 필요시 FTS5 결과도 병합<br />
c. MMR로 다양화, 크로스 인코더 재랭크</p>
  </li>
  <li>
    <p>컨텍스트 패킹<br />
a. final_chunks 개수와 max_context_tokens 안에서 패킹<br />
b. citations에 doc_id, title, page, section, score 포함</p>
  </li>
  <li>
    <p>반환: {context, citations, diagnostics}</p>
  </li>
</ol>

<p>C) 삭제와 재인덱싱<br />
purge_docs: 파일 디렉터리, 텍스트, 청크 jsonl, npy 제거 → SQLite에서 문서·청크 삭제 → FAISS에서 해당 ID 삭제 후 저장<br />
rebuild_index: scope='all'이면 임베딩과 FAISS를 제로부터 리빌드, which로 mtext/en 선택</p>

<p>파일 포맷과 저장 방식<br />
임베딩 저장: NumPy .npy, dtype=float32, shape=[num_chunks, dim]<br />
FAISS 인덱스: IVF_FLAT 또는 HNSW. index.reset(); index.add_with_ids(vecs, global_ids) 후 write_index<br />
글로벌 ID: 64-bit 정수. 권장 매핑은 hash64("{doc_id}:{chunk_id}")로 만들고 SQLite에 gid 컬럼으로 저장<br />
청크 JSONL: 한 줄에 한 청크. 대용량 append/replace가 쉬워 운영 간단</p>

<p>간단 코드 스니펫들</p>

<ol>
  <li>임베딩 npy 저장/로드<br />
import numpy as np</li>
</ol>

<p>def save_embeddings_npy(path, matrix):<br />
path.parent.mkdir(parents=True, exist_ok=True)<br />
np.save(path, matrix.astype(np.float32))</p>

<p>def load_embeddings_npy(path):<br />
return np.load(path).astype(np.float32)</p>

<ol>
  <li>FAISS 인덱스 로드/저장/갱신<br />
import faiss<br />
from pathlib import Path</li>
</ol>

<p>def load_faiss(path, dim):<br />
if Path(path).exists():<br />
return faiss.read_index(str(path))<br />
index = faiss.index_factory(dim, "IVF4096,Flat", faiss.METRIC_INNER_PRODUCT)<br />
quantizer = faiss.IndexFlatIP(dim)<br />
index_ivf = faiss.IndexIVFFlat(quantizer, dim, 4096, faiss.METRIC_INNER_PRODUCT)<br />
return index if isinstance(index, faiss.Index) else index_ivf</p>

<p>def save_faiss(index, path):<br />
Path(path).parent.mkdir(parents=True, exist_ok=True)<br />
faiss.write_index(index, str(path))</p>

<p>def add_embeddings(index, vecs, ids):<br />
faiss.normalize_L2(vecs)<br />
if hasattr(index, "is_trained") and not index.is_trained:<br />
index.train(vecs)<br />
index.add_with_ids(vecs, ids)</p>

<ol>
  <li>SQLite FTS5 삽입<br />
import sqlite3, json</li>
</ol>

<p>def upsert_chunks_sqlite(db_path, doc_id, chunks):<br />
conn = sqlite3.connect(db_path)<br />
c = conn.cursor()<br />
c.execute("DELETE FROM chunks WHERE doc_id=?", (doc_id,))<br />
for ch in chunks:<br />
c.execute("INSERT INTO chunks(chunk_id,doc_id,page,section,content,tokens) VALUES(?,?,?,?,?,?)",<br />
(ch["chunk_id"], doc_id, ch.get("page"), ch.get("section"), ch["content"], ch.get("tokens",0)))<br />
c.execute("INSERT INTO fts_chunks(rowid, content) SELECT rowid, content FROM chunks WHERE doc_id=?", (doc_id,))<br />
conn.commit()<br />
conn.close()</p>

<ol>
  <li>검색 파이프라인 핵심<br />
def search_pipeline(query, top_docs=10, top_chunks_per_doc=5, final_chunks=12):</li>
</ol>

<h1 id="1-쿼리-임베딩">1) 쿼리 임베딩</h1>

<p>q_vec = embed_mtext([query])[0].astype("float32")<br />
faiss.normalize_L2(q_vec.reshape(1,-1))</p>

<h1 id="2-글로벌-청크에서-topn-후보">2) 글로벌 청크에서 topN 후보</h1>

<p>D, I = faiss_index.search(q_vec.reshape(1,-1), 200)<br />
chunk_candidates = gather_chunks_by_ids(I[0], D[0]) # SQLite에서 메타 로드</p>

<h1 id="3-문서-집계--상위-top_docs">3) 문서 집계 → 상위 top_docs</h1>

<p>doc_scores = aggregate_by_doc(chunk_candidates)<br />
doc_top = take_top(doc_scores, top_docs)</p>

<h1 id="4-문서별-재검색">4) 문서별 재검색</h1>

<p>refined = []<br />
for d in doc_top:<br />
refined += search_chunks_in_doc(d.doc_id, q_vec, top_chunks_per_doc)<br />
refined = mmr_diversify(refined, k=final_chunks*4)<br />
ranked = rerank_cross_encoder(query, refined)[:final_chunks]</p>

<h1 id="5-컨텍스트-빌드">5) 컨텍스트 빌드</h1>

<p>context, citations = build_context_and_citations(ranked)<br />
return {"context": context, "citations": citations}</p>

<p>운영·안전 포인트<br />
락킹: indices/index.lock 파일로 파일 잠금 후 인덱스 저장.<br />
원자성: 인덱스 저장은 tmp에 쓴 뒤 rename으로 교체.<br />
버전관리: manifest/embedding_manifest.json에 모델명, 차원, 토큰라이저 버전 기록.<br />
권한: documents.acl_users/acl_roles를 검색 전 필터에 반영.<br />
복구: 청크 JSONL → SQLite 재구축, npy → FAISS 재빌드가 가능하도록 유지.</p>

<p>추천 기본값<br />
embedding 모델: bge-m3, dim=1024<br />
청크 크기: 200~400 토큰, overlap 15%<br />
faiss nprobe: 8~32, IVF lists: 4096부터 시작<br />
검색 폭: top_docs=10, top_chunks_per_doc=5, final_chunks=12, max_context_tokens=6000<br />
하이브리드: use_hybrid=true, FTS5 가중치 0.3, 벡터 0.7</p>

<p>정리<br />
로컬 파일 시스템만으로도 문서 등록부터 인덱싱, 검색까지 충분히 안정적인 RAG 스토어를 만들 수 있다. 핵심은 규칙적인 디렉터리 구조, SQLite+FTS5로의 메타·키워드 처리, FAISS 인덱스의 안전 저장, 그리고 FunctionTool 세트를 통한 일관된 오케스트레이션이다. 다음 단계로 원하면 이 스펙을 바로 실행 가능한 최소 Python 패키지 구조와 CLI까지 포함해 뼈대 코드를 만들어줄 수 있다.</p>

<p class="chat--user">

유저가 사용할 땐 어떤 순서로 해야해?

</p>

<p>다음 순서대로 쓰면 됩니다. 한 번 익히면 흐름이 늘 같습니다: 등록 → 인덱싱 → 검색 → 답변.</p>

<ol>
  <li>최초 1회 설정</li>
</ol>

<ul>
  <li>
    <p>로컬 디렉터리 준비: rag_store/ 하위에 blobs, texts, chunks, embeddings, indices, manifest, locks와 rag_meta.sqlite 생성.</p>
  </li>
  <li>
    <p>임베딩/인덱스 초기화: embedding 모델 선택(bge-m3 등), FAISS 인덱스 파일 생성(indices/faiss_mtext.index).</p>
  </li>
</ul>

<ol>
  <li>문서 등록</li>
</ol>

<ul>
  <li>
    <p>파일을 준비하고 upsert_documents 함수(또는 API/CLI)를 호출합니다.<br />
예시(Python/FunctionTool 호출 파라미터)<br />
{<br />
"items": [<br />
{<br />
"title": "spec_A.pdf",<br />
"source_path": "/path/spec_A.pdf",<br />
"lang_hint": "en",<br />
"tags": ["spec","v1"]<br />
}<br />
],<br />
"reindex": true<br />
}</p>
  </li>
  <li>
    <p>내부 동작: 파일을 blobs/{doc_id}/로 복사하고 documents 테이블에 기록, status='processing', 인덱싱 잡을 큐에 넣음.</p>
  </li>
</ul>

<ol>
  <li>인덱싱 자동 처리</li>
</ol>

<ul>
  <li>
    <p>워커가 텍스트 추출 → 청크 → 임베딩(npy) 저장 → FAISS 인덱스 갱신 → FTS5 업데이트 → status='ready'로 전환.</p>
  </li>
  <li>
    <p>필요하면 describe_doc(doc_id)로 상태 확인. 청크 수, 페이지 수, 임베딩 모델 버전 등을 바로 확인 가능.</p>
  </li>
</ul>

<ol>
  <li>검색과 질의응답</li>
</ol>

<ul>
  <li>
    <p>사용자가 질문을 입력하면, 애플리케이션이 먼저 search_docs를 호출해 컨텍스트를 확보합니다.<br />
권장 파라미터 예:<br />
{<br />
"query": "반품 정책 핵심이 뭐야?",<br />
"filters": { "langs": ["en"], "tags": ["policy"] },<br />
"top_docs": 10,<br />
"top_chunks_per_doc": 5,<br />
"final_chunks": 12,<br />
"use_hybrid": true,<br />
"use_mmr": true,<br />
"use_reranker": true,<br />
"max_context_tokens": 6000<br />
}</p>
  </li>
  <li>
    <p>search_docs가 반환한 context와 citations를 LLM에 넣어 답변 생성.</p>
  </li>
  <li>
    <p>한글 질의로 영어 문서를 찾으려면 다국어 임베딩 또는 translate_query_to_en=true를 사용.</p>
  </li>
</ul>

<ol>
  <li>운영 중 문서 추가/갱신</li>
</ol>

<ul>
  <li>
    <p>새 파일 추가: upsert_documents(items=[…], reindex=true).</p>
  </li>
  <li>
    <p>기존 문서 교체: 같은 doc_id로 upsert하거나 source_path를 업데이트 후 reindex.</p>
  </li>
  <li>
    <p>여러 파일 배치 등록도 동일. 상태가 ‘ready’가 되면 바로 검색에 포함됩니다.</p>
  </li>
</ul>

<ol>
  <li>재인덱싱과 청소</li>
</ol>

<ul>
  <li>
    <p>임베딩 모델 교체나 청크 규칙 변경 시 rebuild_index로 전체 또는 특정 문서를 재빌드.<br />
{<br />
"scope": "all",<br />
"which": "mtext"<br />
}</p>
  </li>
  <li>
    <p>문서 삭제는 purge_docs로 처리. 로컬 파일, 텍스트, 청크, 임베딩, 메타DB, FAISS 참조까지 정리.</p>
  </li>
</ul>

<ol>
  <li>권한과 필터링</li>
</ol>

<ul>
  <li>
    <p>사용자별로 roles/user_id를 애플리케이션이 전달하고, search_docs가 ACL을 반영해 필터링.</p>
  </li>
  <li>
    <p>태그, 언어, 수정일(updated_after)로 검색 범위를 줄여 지연을 안정화.</p>
  </li>
</ul>

<ol>
  <li>장애/품질 관리 팁</li>
</ol>

<ul>
  <li>
    <p>status='error'일 때 describe_doc로 에러 메시지 확인 후 재시도.</p>
  </li>
  <li>
    <p>검색 품질 저하 시 이 순서로 점검: 청크 크기(200~400 토큰), 임베딩 모델 일치, 하이브리드 검색 활성화, 리랭커 적용, MMR 다양화, 최신 가중치 적용.</p>
  </li>
  <li>
    <p>지연이 느리면 top_docs, top_chunks_per_doc, final_chunks를 줄이고, FAISS nprobe 튜닝.</p>
  </li>
</ul>

<p>사용자 경험 관점의 최소 흐름(진짜 실무용 압축본)</p>

<ol>
  <li>
    <p>파일 드래그·드롭 또는 업로드 → upsert_documents(reindex=true)</p>
  </li>
  <li>
    <p>잠시 후 준비 완료 → describe_doc로 ready 확인</p>
  </li>
  <li>
    <p>질문 입력 → 백엔드가 search_docs 호출 → context 수집</p>
  </li>
  <li>
    <p>LLM 호출 → 한국어 답변과 출처 표시</p>
  </li>
  <li>
    <p>추가 파일이 생기면 1로 돌아감</p>
  </li>
</ol>

<p>API/CLI 예시 한 줄씩</p>

<ul>
  <li>
    <p>등록: cli upsert –path /docs/spec.pdf –title "spec" –lang en –tag spec –reindex</p>
  </li>
  <li>
    <p>상태: cli describe –doc-id <uuid></uuid></p>
  </li>
  <li>
    <p>검색: cli search –q "refund policy" –lang en –top-docs 10 –top-chunks 5</p>
  </li>
  <li>
    <p>재빌드: cli rebuild –scope all</p>
  </li>
  <li>
    <p>삭제: cli purge –doc-ids <uuid1>,<uuid2></uuid2></uuid1></p>
  </li>
</ul>

<p>이 흐름을 앱에 녹이면 최종 사용자는 “문서 업로드 → 질문”만 알면 되고, 나머지 전처리·검색·컨텍스트 패킹은 백엔드가 FunctionTool로 수행합니다. 다음 단계로 원하면, 위 순서를 그대로 수행하는 REST 라우트 또는 간단한 CLI 스켈레톤을 만들어 줄게.</p>

        
      </section>

      <footer class="page__meta">
        
        


  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#llm" class="page__taxonomy-item p-category" rel="tag">llm</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2025-11-04T14:13:34+09:00">2025-11-04</time></p>

      </footer>

      

      
  <nav class="pagination">
    
      <a href="/comfyui/comfyui-debug-node-refactor/" class="pagination--pager" title="ComfyUI Debug 노드 리팩터링">Previous</a>
    
    
      <a href="/llm/python-interpreter-tool-calling/" class="pagination--pager" title="Python 코드 실행 Tool Calling">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
    
<div class="page__related">
  
  <h2 class="page__related-title">You may also enjoy</h2>
  <div class="grid__wrapper">
    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2025-08-19-python-repr-vs-str/" rel="permalink">
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <!-- hvppyarchive: remove date icon
        <i class="far fa-calendar-alt" aria-hidden="true"></i> 
        -->
        
        <time datetime="2025-11-12T17:53:43+09:00">2025-11-12</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/python/custom-pyside-filedialog/" rel="permalink">CFileDialog 구현 방법
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <!-- hvppyarchive: remove date icon
        <i class="far fa-calendar-alt" aria-hidden="true"></i> 
        -->
        
        <time datetime="2025-11-12T17:53:38+09:00">2025-11-12</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">

</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/misc/web-korean-keyboard/" rel="permalink">웹 한글 키보드 추가
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <!-- hvppyarchive: remove date icon
        <i class="far fa-calendar-alt" aria-hidden="true"></i> 
        -->
        
        <time datetime="2025-11-07T20:48:31+09:00">2025-11-07</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">

</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/misc/virutual-korean-keyboard/" rel="permalink">untitled
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <!-- hvppyarchive: remove date icon
        <i class="far fa-calendar-alt" aria-hidden="true"></i> 
        -->
        
        <time datetime="2025-11-07T17:47:20+09:00">2025-11-07</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">ChatGPT404 Not Found
</p>
  </article>
</div>

    
  </div>
</div>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
  </ul>
</div>


<div class="page__footer-copyright">&copy; 2013 - 2025 <a href="https://hvppyarchive.github.io">ARCHIVE</a>. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/jekyll-themes/minimal-mistakes/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>






  </body>
</html>
