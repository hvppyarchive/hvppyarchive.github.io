<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.27.1 by Michael Rose
  Copyright 2013-2025 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->

<html lang="en-US" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>vLLM에 pdf 넣는법 - ARCHIVE</title>
<meta name="description" content="">


  <meta name="author" content="hvppyarchive">
  
  <meta property="article:author" content="hvppyarchive">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="ARCHIVE">
<meta property="og:title" content="vLLM에 pdf 넣는법">
<meta property="og:url" content="https://hvppyarchive.github.io/llm/vllm-pdf/">


  <meta property="og:description" content="">







  <meta property="article:published_time" content="2025-11-04T09:34:19+09:00">





  

  


<link rel="canonical" href="https://hvppyarchive.github.io/llm/vllm-pdf/">












<!-- end _includes/seo.html -->


<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
  
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css"></noscript>



    <!-- start custom head snippets -->
<!-- hvppyarchive: add fonts -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Serif+KR:wght@200..900&display=swap" rel="stylesheet">

<!-- insert favicons. use https://realfavicongenerator.net/ -->
 <!-- hvppyarchive: add favicons -->
<link rel="icon" type="image/png" href="/assets/favicon/favicon-96x96.png" sizes="96x96" />
<link rel="icon" type="image/svg+xml" href="/assets/favicon/favicon.svg" />
<link rel="shortcut icon" href="/assets/favicon/favicon.ico" />
<link rel="apple-touch-icon" sizes="180x180" href="/assets/favicon/apple-touch-icon.png" />
<link rel="manifest" href="/assets/favicon/site.webmanifest" />

<!-- end custom head snippets -->

  </head>

  <body class="layout--single" dir="ltr">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          ARCHIVE
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a
                href="/year-archive/"
                
                
              >Posts</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      





<div id="main" role="main">
  


  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="vLLM에 pdf 넣는법">
    <meta itemprop="description" content="">
    <meta itemprop="datePublished" content="2025-11-04T09:34:19+09:00">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">
            <a href="https://hvppyarchive.github.io/llm/vllm-pdf/" itemprop="url">vLLM에 pdf 넣는법
</a>
          </h1>
          

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <!-- hvppyarchive: remove date icon
        <i class="far fa-calendar-alt" aria-hidden="true"></i> 
        -->
        
        <time datetime="2025-11-04T09:34:19+09:00">2025-11-04</time>
      </span>
    

    

    
  </p>


        </header>
      

      <section class="page__content" itemprop="text">
        
        <p class="chat--user">

vllm 서빙하는 모델 gptoss에 pdf 넣는법

</p>

<p>요약 먼저: vLLM으로 서빙 중인 gpt-oss는 “PDF 파일을 직접 넣어 처리”하는 인터페이스가 없습니다. vLLM OpenAI 호환 서버는 텍스트/토큰 스트림을 받는 API이고, gpt-oss는 공개된 가이드 기준 텍스트 전용 모델이라서(이미지·PDF 업로드 파라미터 없음) PDF는 사전 전처리(텍스트 추출→청소→청크→검색/RAG)로 넣어야 합니다. <a href="https://docs.vllm.ai/en/v0.6.0/serving/openai_compatible_server.html?utm_source=chatgpt.com">docs.vllm.ai+4</a></p>

<p>아래는 “PDF를 gpt-oss(vLLM 서버)로 넣는” 가장 흔한 구현 레시피입니다. 단계별로 그대로 따라하면 됩니다.</p>

<ol>
  <li>vLLM 서버로 gpt-oss 띄우기</li>
</ol>

<ul>
  <li>CUDA 환경에서 예시:</li>
</ul>

<div class="language-css highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">pip</span> <span class="nt">install</span> <span class="nt">vllm</span>
<span class="nt">python</span> <span class="nt">-m</span> <span class="nt">vllm</span><span class="nc">.entrypoints.openai.api_server</span> <span class="err">\</span>
  <span class="nt">--model</span> <span class="nt">openai</span><span class="o">/</span><span class="nt">gpt-oss-20b</span> <span class="err">\</span>
  <span class="nt">--max-model-len</span> <span class="err">32768</span> <span class="err">\</span>
  <span class="nt">--port</span> <span class="err">8000</span>
</code></pre></div></div>

<p>서버는 OpenAI 호환 Chat/Completions 엔드포인트를 제공합니다. 기본적으로 파일 업로드 필드는 없고 문자열 프롬프트만 받습니다. <a href="https://docs.vllm.ai/en/v0.6.0/serving/openai_compatible_server.html?utm_source=chatgpt.com">docs.vllm.ai</a></p>

<ol>
  <li>PDF → 텍스트 추출</li>
</ol>

<ul>
  <li>
    <p>Python 도구 예: pypdf, pdfminer.six, unstructured, docling 등.</p>
  </li>
  <li>
    <p>표·머리글·각주가 섞여 있으면 docling/Unstructured 계열이 품질이 더 낫습니다. 추출 후 다음을 꼭 하세요.<br />
• 페이지 헤더/푸터 제거, 공백 정규화, 본문 병합<br />
• 너무 긴 표는 CSV로 따로 저장(필요시)<br />
관련 이슈와 파이프라인 선택은 커뮤니티/라이브러리 이슈를 참고하세요. <a href="https://github.com/docling-project/docling/issues/1365?utm_source=chatgpt.com">GitHub</a></p>
  </li>
</ul>

<ol>
  <li>청크 나누기</li>
</ol>

<ul>
  <li>
    <p>800~1500자(또는 200~400 토큰) 단위로 자르고, 문단 경계를 최대한 유지합니다.</p>
  </li>
  <li>
    <p>각 청크에 원문 페이지/섹션 메타데이터를 붙입니다.</p>
  </li>
</ul>

<ol>
  <li>임베딩 생성 → 벡터DB 적재</li>
</ol>

<ul>
  <li>
    <p>임베딩 모델(예: bge-m3, jina-embeddings, sentence-transformers 등)로 각 청크를 벡터화하고, FAISS/Chroma/PGVector에 넣습니다.</p>
  </li>
  <li>
    <p>이 단계는 LLM(gpt-oss/vLLM)과 분리된 별도 과정입니다. vLLM 자체는 임베딩을 만들지 않습니다. RAG 패턴 예시는 다수의 vLLM 배포 예제에서도 동일합니다. <a href="https://validatedpatterns.io/patterns/rag-llm-gitops/?utm_source=chatgpt.com">validatedpatterns.io+1</a></p>
  </li>
</ul>

<ol>
  <li>질의 시 검색(RAG) → 프롬프트 구성 → vLLM에 전달</li>
</ol>

<ul>
  <li>
    <p>유저 질문을 벡터 검색으로 top-k 청크와 매칭</p>
  </li>
  <li>
    <p>아래 System/Assistant 컨텍스트에 “검색 결과”를 넣고, gpt-oss의 권장 포맷(하모니 포맷)을 지키며 요청을 보냅니다. gpt-oss는 OpenAI 호환 서버에서 잘 동작하도록 가이드가 있습니다. <a href="https://cookbook.openai.com/articles/gpt-oss/run-vllm?utm_source=chatgpt.com">cookbook.openai.com+1</a></p>
  </li>
</ul>

<p>실전 코드 스니펫</p>

<p>A. PDF 텍스트 추출/청크(간단 예시)</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">pypdf</span> <span class="kn">import</span> <span class="n">PdfReader</span>

<span class="k">def</span> <span class="nf">extract_pdf_text</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
    <span class="n">reader</span> <span class="o">=</span> <span class="nc">PdfReader</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
    <span class="n">pages</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">reader</span><span class="p">.</span><span class="n">pages</span><span class="p">):</span>
        <span class="n">t</span> <span class="o">=</span> <span class="n">p</span><span class="p">.</span><span class="nf">extract_text</span><span class="p">()</span> <span class="ow">or</span> <span class="sh">""</span>
        <span class="n">pages</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">page</span><span class="sh">"</span><span class="p">:</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">:</span> <span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="nf">split</span><span class="p">())})</span>
    <span class="k">return</span> <span class="n">pages</span>

<span class="k">def</span> <span class="nf">chunk_text</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">max_chars</span><span class="o">=</span><span class="mi">1200</span><span class="p">,</span> <span class="n">overlap</span><span class="o">=</span><span class="mi">150</span><span class="p">):</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">start</span> <span class="o">=</span> <span class="p">[],</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="nf">len</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
        <span class="n">end</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">text</span><span class="p">),</span> <span class="n">start</span> <span class="o">+</span> <span class="n">max_chars</span><span class="p">)</span>
        <span class="n">out</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">text</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">])</span>
        <span class="n">start</span> <span class="o">=</span> <span class="n">end</span> <span class="o">-</span> <span class="n">overlap</span>
        <span class="k">if</span> <span class="n">start</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span> <span class="n">start</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">return</span> <span class="n">out</span>

<span class="n">pages</span> <span class="o">=</span> <span class="nf">extract_pdf_text</span><span class="p">(</span><span class="sh">"</span><span class="s">sample.pdf</span><span class="sh">"</span><span class="p">)</span>
<span class="n">chunks</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">page</span> <span class="ow">in</span> <span class="n">pages</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nf">chunk_text</span><span class="p">(</span><span class="n">page</span><span class="p">[</span><span class="sh">"</span><span class="s">text</span><span class="sh">"</span><span class="p">]):</span>
        <span class="n">chunks</span><span class="p">.</span><span class="nf">append</span><span class="p">({</span><span class="sh">"</span><span class="s">page</span><span class="sh">"</span><span class="p">:</span> <span class="n">page</span><span class="p">[</span><span class="sh">"</span><span class="s">page</span><span class="sh">"</span><span class="p">],</span> <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span> <span class="n">c</span><span class="p">})</span>
</code></pre></div></div>

<p>B. 임베딩→FAISS 적재(예시: sentence-transformers)</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">sentence_transformers</span> <span class="kn">import</span> <span class="n">SentenceTransformer</span>
<span class="kn">import</span> <span class="n">faiss</span><span class="p">,</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">model</span> <span class="o">=</span> <span class="nc">SentenceTransformer</span><span class="p">(</span><span class="sh">"</span><span class="s">sentence-transformers/all-MiniLM-L6-v2</span><span class="sh">"</span><span class="p">)</span>
<span class="n">emb</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encode</span><span class="p">([</span><span class="n">c</span><span class="p">[</span><span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">chunks</span><span class="p">],</span> <span class="n">convert_to_numpy</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">normalize_embeddings</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">index</span> <span class="o">=</span> <span class="n">faiss</span><span class="p">.</span><span class="nc">IndexFlatIP</span><span class="p">(</span><span class="n">emb</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">index</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="n">emb</span><span class="p">)</span>
</code></pre></div></div>

<p>C. 검색→vLLM(gpt-oss) 호출(OpenAI 호환)</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span><span class="p">,</span> <span class="n">requests</span><span class="p">,</span> <span class="n">json</span>

<span class="k">def</span> <span class="nf">search</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">topk</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">encode</span><span class="p">([</span><span class="n">query</span><span class="p">],</span> <span class="n">convert_to_numpy</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">normalize_embeddings</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">scores</span><span class="p">,</span> <span class="n">idx</span> <span class="o">=</span> <span class="n">index</span><span class="p">.</span><span class="nf">search</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">topk</span><span class="p">)</span>
    <span class="n">ctx</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">idx</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="n">ctx</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">[p.</span><span class="si">{</span><span class="n">chunks</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="sh">'</span><span class="s">page</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="s">]</span><span class="se">\n</span><span class="si">{</span><span class="n">chunks</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="sh">'</span><span class="s">content</span><span class="sh">'</span><span class="p">]</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="sh">"</span><span class="se">\n\n</span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span><span class="n">ctx</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">ask_vllm</span><span class="p">(</span><span class="n">query</span><span class="p">):</span>
    <span class="n">context</span> <span class="o">=</span> <span class="nf">search</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">topk</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span><span class="sh">"</span><span class="s">system</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span><span class="sh">"</span><span class="s">You are a helpful assistant. Use the provided CONTEXT; if missing, say you don</span><span class="sh">'</span><span class="s">t know.</span><span class="sh">"</span><span class="p">},</span>
        <span class="p">{</span><span class="sh">"</span><span class="s">role</span><span class="sh">"</span><span class="p">:</span><span class="sh">"</span><span class="s">user</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">:</span><span class="sa">f</span><span class="sh">"</span><span class="s">CONTEXT:</span><span class="se">\n</span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="se">\n\n</span><span class="s">QUESTION:</span><span class="se">\n</span><span class="si">{</span><span class="n">query</span><span class="si">}</span><span class="sh">"</span><span class="p">}</span>
    <span class="p">]</span>
    <span class="n">resp</span> <span class="o">=</span> <span class="n">requests</span><span class="p">.</span><span class="nf">post</span><span class="p">(</span>
        <span class="sh">"</span><span class="s">http://localhost:8000/v1/chat/completions</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">headers</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">Content-Type</span><span class="sh">"</span><span class="p">:</span><span class="sh">"</span><span class="s">application/json</span><span class="sh">"</span><span class="p">},</span>
        <span class="n">data</span><span class="o">=</span><span class="n">json</span><span class="p">.</span><span class="nf">dumps</span><span class="p">({</span><span class="sh">"</span><span class="s">model</span><span class="sh">"</span><span class="p">:</span><span class="sh">"</span><span class="s">openai/gpt-oss-20b</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">messages</span><span class="sh">"</span><span class="p">:</span><span class="n">messages</span><span class="p">,</span><span class="sh">"</span><span class="s">temperature</span><span class="sh">"</span><span class="p">:</span><span class="mf">0.2</span><span class="p">})</span>
    <span class="p">).</span><span class="nf">json</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">resp</span><span class="p">[</span><span class="sh">"</span><span class="s">choices</span><span class="sh">"</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="sh">"</span><span class="s">message</span><span class="sh">"</span><span class="p">][</span><span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">]</span>

<span class="nf">print</span><span class="p">(</span><span class="nf">ask_vllm</span><span class="p">(</span><span class="sh">"</span><span class="s">이 PDF에서 반품 정책 핵심 요약해줘</span><span class="sh">"</span><span class="p">))</span>
</code></pre></div></div>

<p>D. curl로 간단 테스트</p>
<div class="language-swift highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">curl</span> <span class="nv">http</span><span class="p">:</span><span class="c1">//localhost:8000/v1/chat/completions \</span>
  <span class="o">-</span><span class="kt">H</span> <span class="s">"Content-Type: application/json"</span> <span class="p">\</span>
  <span class="o">-</span><span class="n">d</span> <span class="err">'</span><span class="p">{</span>
    <span class="s">"model"</span><span class="p">:</span> <span class="s">"openai/gpt-oss-20b"</span><span class="p">,</span>
    <span class="s">"messages"</span><span class="p">:</span> <span class="p">[</span>
      <span class="p">{</span><span class="s">"role"</span><span class="p">:</span><span class="s">"system"</span><span class="p">,</span><span class="s">"content"</span><span class="p">:</span><span class="s">"Answer using the context."</span><span class="p">},</span>
      <span class="p">{</span><span class="s">"role"</span><span class="p">:</span><span class="s">"user"</span><span class="p">,</span><span class="s">"content"</span><span class="p">:</span><span class="s">"CONTEXT:</span><span class="se">\n</span><span class="s">&lt;여기에 검색으로 모은 PDF 청크&gt;</span><span class="se">\n\n</span><span class="s">QUESTION:</span><span class="se">\n</span><span class="s">요약해줘"</span><span class="p">}</span>
    <span class="p">],</span>
    <span class="s">"temperature"</span><span class="p">:</span> <span class="mf">0.2</span>
  <span class="p">}</span><span class="err">'</span>
</code></pre></div></div>

<p>자주 묻는 포인트</p>

<ul>
  <li>
    <p>왜 PDF 파일 자체를 바로 못 넣나?<br />
vLLM의 OpenAI 호환 서버는 파일 업로드/스토리지 기능이 아니라 텍스트 프롬프트 API입니다. 즉, 파일은 애플리케이션 레이어에서 처리해서 텍스트(또는 검색된 컨텍스트)로 바꾼 다음 모델에 넣어야 합니다. <a href="https://docs.vllm.ai/en/v0.6.0/serving/openai_compatible_server.html?utm_source=chatgpt.com">docs.vllm.ai</a></p>
  </li>
  <li>
    <p>gpt-oss가 PDF나 이미지까지 직접 읽는 멀티모달인가?<br />
공개 가이드 기준 gpt-oss는 텍스트 중심의 오픈 웨이트 패밀리로 소개되었습니다. 멀티모달 입력을 직접 받는 형태가 아니라, 텍스트로 컨텍스트를 주는 방식이 표준입니다. <a href="https://openai.com/index/introducing-gpt-oss/?utm_source=chatgpt.com">OpenAI+2</a></p>
  </li>
  <li>
    <p>“PDF 질의응답” 정밀도를 더 높이려면?<br />
청크 크기/오버랩 최적화, 표/코드 블록 보존, 섹션별 재순위화(re-ranking), “출처 페이지 표시”를 프롬프트에 강제, 그리고 답변 근거를 따로 출력하도록 규칙을 넣으면 품질이 안정됩니다. vLLM 레시피/쿠크북의 gpt-oss 운영 가이드도 함께 보세요. <a href="https://docs.vllm.ai/projects/recipes/en/latest/OpenAI/GPT-OSS.html?utm_source=chatgpt.com">docs.vllm.ai+1</a></p>
  </li>
</ul>

<p>한 줄 정리: vLLM+gpt-oss로 PDF를 “넣는” 것은 PDF를 텍스트로 전처리하고, 검색 기반(RAG)으로 관련 청크를 추려 OpenAI 호환 API에 문자열로 보내는 것입니다. 이 흐름을 코드와 같이 구성하면 바로 운영에 올릴 수 있습니다.</p>

<p class="chat--user">

openai agent 라는 패키지가 있는데 여기에 그런 레이어가 포함되어있나?

</p>

<p>짧은 결론부터 말하면, “openai agent”로 알려진 OpenAI Agents SDK는 에이전트 오케스트레이션(툴 호출, 세션/메모리, 핸드오프, 가드레일 등)을 제공하지만 PDF 파싱·청크·임베딩·벡터스토어 적재 같은 “문서 RAG 인입 레이어”는 기본 내장되어 있지 않습니다. 다만 OpenAI 모델을 쓸 때는 호스티드 도구(FileSearchTool 등)를 통해 OpenAI Vector Stores에 올려둔 문서를 검색하는 방식은 지원합니다. 자체 vLLM+gpt-oss로 돌릴 때는 그 레이어를 직접 붙여야 합니다. <a href="https://openai.github.io/openai-agents-python/">openai.github.io+1</a></p>

<p>핵심 포인트를 정리하면 아래와 같습니다.</p>

<ol>
  <li>
    <p>SDK가 해주는 일<br />
에이전트 루프, 함수툴(FunctionTool), 세션/트레이싱, 핸드오프, MCP(Model Context Protocol) 연결 등 “오케스트레이션”이 주역할입니다. PDF를 읽어 텍스트로 만들고 임베딩을 뽑아 벡터DB에 적재하는 파이프라인은 앱 쪽에서 구성해야 합니다. <a href="https://openai.github.io/openai-agents-python/">openai.github.io+1</a></p>
  </li>
  <li>
    <p>OpenAI 호스티드 도구가 있을 때<br />
OpenAI Responses 모델을 쓰면 FileSearchTool, WebSearchTool, Code Interpreter 같은 “호스티드 툴”을 바로 붙일 수 있습니다. 이 경우 PDF는 OpenAI Vector Stores API로 업로드·처리하고, 에이전트는 FileSearchTool로 검색합니다. SDK 자체가 PDF를 파싱해주진 않고, 벡터스토어도 OpenAI 쪽 리소스를 이용하는 구성입니다. <a href="https://openai.github.io/openai-agents-python/tools/">openai.github.io</a></p>
  </li>
  <li>
    <p>gpt-oss(+vLLM)와 함께 쓰는 경우<br />
vLLM의 OpenAI-호환 엔드포인트로 에이전트의 모델 클라이언트를 바꿔치기 할 수 있습니다. 다만 이때는 OpenAI 호스티드 도구(FileSearchTool 등)가 그대로 동작하지 않거나 전제가 바뀌므로, 직접 함수툴로 “검색/RAG”를 구현하거나 MCP 서버(자체 문서 검색 서버)를 붙여서 문서 검색을 노출하는 패턴이 일반적입니다. <a href="https://cookbook.openai.com/articles/gpt-oss/run-vllm?utm_source=chatgpt.com">cookbook.openai.com+2</a></p>
  </li>
  <li>
    <p>최소 구현 스케치</p>
  </li>
</ol>

<ul>
  <li>
    <p>PDF 인입: pypdf/unstructured로 텍스트 추출 → 청크 → 임베딩(bge/jina/sentence-transformers) → FAISS/Chroma 적재</p>
  </li>
  <li>
    <p>에이전트 툴: search(query) 같은 Python 함수툴을 정의해 top-k 청크를 반환하도록 구현</p>
  </li>
  <li>
    <p>에이전트: Agent(tools=[search])로 등록, 모델은 vLLM의 Chat Completions/Responses 호환 엔드포인트로 지정</p>
  </li>
  <li>
    <p>OpenAI 모델로 바꿀 때만 FileSearchTool을 사용해 OpenAI Vector Store를 조회 가능<br />
이 구성은 SDK 문서의 “Tools”, “Context management”, “MCP” 섹션 흐름과 딱 맞습니다. <a href="https://openai.github.io/openai-agents-python/tools/">openai.github.io+2</a></p>
  </li>
</ul>

<ol>
  <li>이름 혼동 주의<br />
Assistants API는 Responses API로 대체되는 흐름이며, Agents SDK는 이 Responses 기반에서 돌아갑니다. 최신 가이드와 레퍼런스를 참고해서 구성하세요. <a href="https://www.reuters.com/technology/artificial-intelligence/openai-launches-new-developer-tools-chinese-ai-startups-gain-ground-2025-03-11/?utm_source=chatgpt.com">reuters.com+1</a></li>
</ol>

<p>정리</p>

<ul>
  <li>포함되어 있는가? 에이전트 오케스트레이션은 포함, “PDF→RAG 인입 레이어”는 미포함. OpenAI 모델일 때만 FileSearchTool로 OpenAI Vector Stores를 조회할 수 있음. vLLM+gpt-oss에서는 직접 함수툴이나 MCP로 붙여야 합니다. <a href="https://openai.github.io/openai-agents-python/tools/">openai.github.io+1</a></li>
</ul>

<p>원하시면 지금 쓰시는 vLLM 서버 주소와 선호 임베딩/벡터DB를 가정해, Agents SDK용 최소 예제(함수툴 기반 검색 + vLLM 연결) 스니펫을 바로 만들어 드립니다.</p>

<p class="chat--user">

openai vector store는 어떤식으로 구성되어있어?

</p>

<p>좋아요 — “OpenAI Vector Store”가 대체로 어떻게 구성되어 있는지(=아키텍처, 주요 컴포넌트, API 흐름) 같이 살펴봅시다. (약간 기술적이긴 하지만, 전체 그림을 이해하면 이후 구현할 때 훨씬 수월해요.)</p>

<hr />

<h2 id="1-기본-개념">1. 기본 개념</h2>

<p>Vector Store는 간단히 말해 “문서/청크(텍스트 단위)를 임베딩 벡터로 변환해 저장하고, 이후 검색 가능하도록 조회하는 저장소”입니다. OpenAI 쪽에서 제공하는 API는 문서를 업로드 → 임베딩/청크화(내부 처리) → 벡터저장소 생성 및 파일 바인딩 → 검색 질의(Query) → 관련 청크 반환하는 흐름을 지원합니다. <a href="https://platform.openai.com/docs/assistants/tools/file-search/vector-stores?utm_source=chatgpt.com">platform.openai.com+3</a></p>

<p>즉, 내부적으로는 다음 요소들이 있습니다:</p>

<ul>
  <li>
    <p>문서 파일(예: PDF, 텍스트) 업로드 및 처리된 파일 배치(file batch)</p>
  </li>
  <li>
    <p>벡터 변환(임베딩) + 청크(metadata 포함)</p>
  </li>
  <li>
    <p>저장소(Vector store) 객체: 여러 파일/청크들을 묶음</p>
  </li>
  <li>
    <p>검색 API: 벡터 유사도 기반 조회 → 반환된 청크와 메타정보</p>
  </li>
</ul>

<hr />

<h2 id="2-openai-api-상의-구성-요소">2. OpenAI API 상의 구성 요소</h2>

<p>OpenAI 문서를 기준으로 주요 리소스(resource)들이 어떻게 연결되는지요:</p>

<ul>
  <li>
    <p><strong>Vector Store 리소스</strong> : <code class="language-plaintext highlighter-rouge">/v1/vector_stores</code> 엔드포인트로 만들 수 있음. <a href="https://platform.openai.com/docs/api-reference/vector-stores?utm_source=chatgpt.com">platform.openai.com+1</a></p>
  </li>
  <li>
    <p><strong>파일 배치(File Batches)</strong> : 업로드된 파일이 청크/벡터화되어 저장되는 단위. <code class="language-plaintext highlighter-rouge">/v1/vector_stores/{vector_store_id}/file_batches</code> 같은 구조 있음. <a href="https://platform.openai.com/docs/api-reference/vector-stores-file-batches?utm_source=chatgpt.com">platform.openai.com</a></p>
  </li>
  <li>
    <p><strong>파일(파일 업로드)</strong> : <code class="language-plaintext highlighter-rouge">/v1/vector_stores/{vs_id}/files</code> 엔드포인트에서 파일을 추가할 수 있음. <a href="https://platform.openai.com/docs/api-reference/vector-stores-files?utm_source=chatgpt.com">platform.openai.com</a></p>
  </li>
  <li>
    <p><strong>검색(Search)</strong> : 저장된 Vector Store에 대해 “유사벡터 기반 검색”을 요청 가능 (예: 파일 검색, 청크 검색) <a href="https://platform.openai.com/docs/assistants/tools/file-search/vector-stores?utm_source=chatgpt.com">platform.openai.com+1</a></p>
  </li>
</ul>

<hr />

<h2 id="3-흐름-예시로-보기">3. 흐름 예시로 보기</h2>

<p>아래처럼 단계별 구성됩니다:</p>

<ol>
  <li>
    <p>파일 업로드</p>

    <ul>
      <li>예: PDF를 업로드 → OpenAI 측에서 자동으로 청크화/임베딩/저장 준비 → 파일 배치 생성</li>
    </ul>
  </li>
  <li>
    <p>Vector Store 생성</p>

    <ul>
      <li>저장소 이름, 설명 등의 메타데이터 제공 → 생성</li>
    </ul>
  </li>
  <li>
    <p>파일을 Vector Store에 바인딩(=파일 추가)</p>

    <ul>
      <li>생성한 Vector Store ID에 파일 배치/파일을 업로드하는 API 호출</li>
    </ul>
  </li>
  <li>
    <p>검색 요청</p>

    <ul>
      <li>
        <p>사용자가 질의(query)를 보내면, API는 임베딩 생성 → 저장된 벡터들과 유사도 비교 → 관련 청크 반환</p>
      </li>
      <li>
        <p>반환된 청크를 기반으로 응답 생성(LLM 호출 전처리 역할)</p>
      </li>
    </ul>
  </li>
  <li>
    <p>LLM 호출(별도)</p>

    <ul>
      <li>검색된 청크는 LLM(예: gpt-모델)에 컨텍스트로 전달 → 답변 생성</li>
    </ul>
  </li>
</ol>

<hr />

<h2 id="4-주요-특성-및-한계">4. 주요 특성 및 한계</h2>

<ul>
  <li>
    <p>파일/문서 메타데이터는 비교적 제한적: 업로드된 파일이 얼마나 긴지(token count, char count) 등의 정보 부족하다는 사용자의 피드백이 있음. <a href="https://www.reddit.com/r/OpenAI/comments/1lacgqe/openais_vector_store_api_is_missing_basic/?utm_source=chatgpt.com">Reddit</a></p>
  </li>
  <li>
    <p>청크화 + 임베딩 + 저장/관리 과정을 “사용자제어” 하려면 OpenAI 호스티드 기능보다는 자체 벡터DB + 임베딩 워크플로우가 더 유연함</p>
  </li>
  <li>
    <p>검색 품질은 임베딩 모델(어떤 모델을 썼는가) + 청크 단위/문서 구조 설계에 크게 좌우됨</p>
  </li>
</ul>

<hr />

<h2 id="5-gpt-ossvllm-활용-시-고려사항">5. gpt-oss/vLLM 활용 시 고려사항</h2>

<p>당신이 vLLM + gpt-oss 조합으로 쓰는 상황이라면, OpenAI Vector Store API를 그대로 의존하기보다는 다음과 같은 변형이 필요할 수 있어요:</p>

<ul>
  <li>
    <p>OpenAI Vector Store API를 쓰고 싶으면 OpenAI 인증/요금/호스팅 구조 따라야 함</p>
  </li>
  <li>
    <p>또는 자체 벡터DB(예: FAISS, Milvus 등) + 임베딩 모델 + 검색 함수툴을 만들어서 “유사 기능”을 구현</p>
  </li>
  <li>
    <p>Agents SDK나 워크플로우에서는 “검색 툴(tool)”이 이 저장소와 연결됨: 사용자가 질의하면 검색툴이 관련 청크를 꺼내오고, LLM에게 문맥으로 제공</p>
  </li>
</ul>

<hr />

<p>필요하시다면, OpenAI Vector Store API를 실제로 호출하는 예제 코드(Python) 만들어 드릴 수 있어요—특히 파일 업로드 → 검색까지 흐름. 그렇게 할까요?</p>

<p class="chat--user">

마지막에 말한 검색툴은 자동으로 동작하는건지 llm이 요청하면 동작하는건지 궁금해

</p>

<p>아주 핵심을 짚으셨습니다.<br />
OpenAI의 “검색 툴(FileSearchTool)”은 <strong>자동으로 동작하지 않습니다</strong>.<br />
LLM이 “필요하다고 판단할 때” — 즉, <strong>모델이 툴 호출을 생성하면</strong> — 그때 실행됩니다.</p>

<p>이걸 조금 더 세밀하게 뜯어보면 이렇게 됩니다.</p>

<hr />

<h3 id="1-에이전트의-기본-구조">1. 에이전트의 기본 구조</h3>

<p>OpenAI Agents SDK(혹은 Responses API)에서 에이전트는 다음 두 가지를 갖습니다.</p>

<ul>
  <li>
    <p><strong>LLM 모델</strong> : gpt-4o, gpt-4o-mini 등</p>
  </li>
  <li>
    <p><strong>툴(tool)</strong> : 함수 호출(Function Tool), FileSearchTool, WebSearchTool, CodeInterpreter 등</p>
  </li>
</ul>

<p>에이전트는 LLM에게 “너는 이런 툴들을 쓸 수 있다”고 알려주고,<br />
대화 중에 LLM이 판단해 특정 툴을 호출할 때만 실제 실행 로직이 수행됩니다.</p>

<hr />

<h3 id="2-툴-호출-과정">2. 툴 호출 과정</h3>

<p>모델이 내부적으로 이런 구조의 응답을 냅니다.</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"function_call"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"name"</span><span class="p">:</span><span class="w"> </span><span class="s2">"search"</span><span class="p">,</span><span class="w">
  </span><span class="nl">"arguments"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="nl">"query"</span><span class="p">:</span><span class="w"> </span><span class="s2">"계약서 요약"</span><span class="w"> </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>SDK는 이걸 감지하고 <strong>search() 함수</strong> (혹은 FileSearchTool)을 호출합니다.<br />
그 결과를 받아 모델에게 다시 전달하면, 모델은 그 결과를 바탕으로 <strong>최종 답변</strong> 을 만듭니다.</p>

<p>즉, LLM이 툴 사용을 “결정”하고, 호출은 SDK가 “대리 실행”합니다.</p>

<hr />

<h3 id="3-자동성과-자율성의-정도">3. 자동성과 자율성의 정도</h3>

<ul>
  <li>
    <p><strong>자동 실행 아님</strong> : 사용자가 “검색을 꼭 하라”고 지정하지 않으면, 모델이 문맥상 필요하다고 판단해야 호출합니다.</p>
  </li>
  <li>
    <p><strong>프롬프트 설계 가능</strong> : system prompt 에 “파일 내용을 참조할 때는 항상 FileSearchTool을 사용하라”처럼 명시해 주면 호출 확률이 높아집니다.</p>
  </li>
  <li>
    <p><strong>Agent Loop</strong> : SDK가 이 과정을 자동 반복해줍니다. 모델이 툴 호출 → 실행 → 결과 전달 → 다시 모델 호출 … 최종 답변이 완성될 때까지 루프를 돕습니다.</p>
  </li>
</ul>

<hr />

<h3 id="4-예시로-보는-흐름">4. 예시로 보는 흐름</h3>

<p>사용자: “이 계약서 요약해줘.”</p>

<ol>
  <li>
    <p>모델은 FileSearchTool이 등록되어 있음을 알고 있음.</p>
  </li>
  <li>
    <p>LLM이 내부적으로 판단: “파일에서 내용 가져와야 하네.”</p>
  </li>
  <li>
    <p>LLM → <code class="language-plaintext highlighter-rouge">function_call: file_search(query="계약서 요약")</code></p>
  </li>
  <li>
    <p>SDK → 실제 Vector Store 검색 수행</p>
  </li>
  <li>
    <p>검색 결과를 모델에게 다시 입력</p>
  </li>
  <li>
    <p>모델 → 검색 결과를 요약한 최종 답변 생성</p>
  </li>
</ol>

<hr />

<h3 id="5-vllm--gpt-oss-조합에서는">5. vLLM + gpt-oss 조합에서는</h3>

<p>vLLM은 OpenAI의 Responses API 내부 오케스트레이션 기능이 없습니다.<br />
따라서 “자동 툴 호출”을 구현하려면 SDK가 필요합니다.<br />
직접 서버나 에이전트 루프를 만들고, 모델의 함수 호출 결과(JSON)를 해석해 수동으로 “search() → 결과 → 모델 재호출”을 구현해야 합니다.</p>

<hr />

<p>정리하자면:<br />
<strong>FileSearchTool은 모델이 필요하다고 판단할 때만 호출되고, SDK가 실행을 중계합니다.</strong><br />
즉, “모델이 요청하면 자동으로 동작한다”는 표현이 가장 정확합니다.</p>

<p>다음 단계로, 원하신다면 vLLM + gpt-oss 조합에서 이 “툴 호출 루프”를 흉내내는 최소 예제(모델 → function_call → search → 재호출)를 만들어 드릴 수 있습니다.</p>

<p class="chat--user">

영어로 작성된 문서 rag를 한글로 쿼리할 수 있어?

</p>

<p>가능합니다. 방법은 크게 두 가지 계열입니다. 한글 쿼리 → 영어 문서 RAG를 지원하려면 “다국어 임베딩”을 쓰거나 “번역 레이어”를 끼우면 됩니다.</p>

<ol>
  <li>다국어 임베딩으로 직접 검색</li>
</ol>

<ul>
  <li>
    <p>핵심: 영어 문서 청크와 한국어 쿼리를 같은 임베딩 공간으로 매핑.</p>
  </li>
  <li>
    <p>모델 예: bge-m3, e5-multilingual, LaBSE, mContriever 등.</p>
  </li>
  <li>
    <p>장점: 번역 없이도 개념 맞춤. 고유명사·전문용어에 강함.</p>
  </li>
  <li>
    <p>단점: 임베딩 품질·차원 수가 커서 메모리↑. 영어 전용 임베딩 대비 약간의 정밀도 손실이 있을 수 있음.</p>
  </li>
</ul>

<ol>
  <li>번역 후 검색(translate-then-retrieve)</li>
</ol>

<ul>
  <li>
    <p>파이프라인: 한글 쿼리 → 영어로 번역 → 영어 전용 임베딩(예: e5-large, bge-large-en 등)으로 검색 → 답변은 한글로 생성.</p>
  </li>
  <li>
    <p>장점: 영어 전용 임베딩/리랭커 품질을 그대로 활용 가능.</p>
  </li>
  <li>
    <p>단점: 번역 오류가 쿼리의 핵심 토픽을 비트는 경우가 있음. 엔티티(제품명, 표기) 주의.</p>
  </li>
</ul>

<p>권장 설계(현업 안정판)</p>

<ul>
  <li>
    <p>인덱싱: 영어 문서를 문단/문장 단위로 청크, 메타데이터(페이지, 섹션, 언어=en) 저장.</p>
  </li>
  <li>
    <p>임베딩: 다국어 임베딩 인덱스 1개는 반드시 구축. 여력이 되면 영어 전용 인덱스를 추가로 구성해 듀얼 인덱스 검색.</p>
  </li>
  <li>
    <p>검색 전략: 한글 쿼리에 대해</p>

    <ol>
      <li>
        <p>다국어 인덱스에서 top-k</p>
      </li>
      <li>
        <p>영어 전용 인덱스(쿼리 번역)에서도 top-k</p>
      </li>
      <li>
        <p>두 결과를 점수 정규화 후 병합</p>
      </li>
    </ol>
  </li>
  <li>
    <p>리랭킹: 다국어 리랭커(bge-reranker-v2-m3 같은 범용 다국어 리랭커)로 상위 50→10 재정렬.</p>
  </li>
  <li>
    <p>하이브리드: BM25(키워드) + 벡터 점수 가중 평균. 고유명사 매칭 강화.</p>
  </li>
  <li>
    <p>생성: gpt-oss(vLLM)에 “출처 표시, 영어 인용문은 원문 유지, 최종 답변은 한국어”를 시스템 규칙으로 주입.</p>
  </li>
</ul>

<p>간단 파이프라인 예시(핵심 로직만)</p>

<ul>
  <li>
    <p>인덱싱</p>

    <ol>
      <li>
        <p>PDF → 텍스트 추출 → 청크</p>
      </li>
      <li>
        <p>multilingual_embed(chunks) → FAISS/Chroma/PGVector에 저장</p>
      </li>
      <li>
        <p>선택: english_embed(chunks)로 영어 전용 인덱스도 병행 구축</p>
      </li>
    </ol>
  </li>
  <li>
    <p>질의</p>

    <ol>
      <li>
        <p>ko_query → multilingual_embed(ko_query)로 검색</p>
      </li>
      <li>
        <p>ko_query → en_query로 번역 → english_embed(en_query)로 검색</p>
      </li>
      <li>
        <p>결과 병합 → 다국어 리랭킹</p>
      </li>
      <li>
        <p>상위 n개를 컨텍스트로 vLLM에 넣고 한국어로 답변 생성</p>
      </li>
    </ol>
  </li>
</ul>

<p>프롬프트 팁</p>

<ul>
  <li>
    <p>시스템 규칙에 “문헌은 영어 원문을 그대로 인용, 설명은 한국어로 작성, 출처 페이지/섹션을 함께 적시”를 명시.</p>
  </li>
  <li>
    <p>다국어 혼용 문서일 경우 “언어 감지 → 쿼리와 다른 언어라도 동일 개념이면 포함” 같은 지침을 추가.</p>
  </li>
</ul>

<p>실무 체크리스트</p>

<ul>
  <li>
    <p>토크나이저 길이: 청크는 200~400토큰(문단 경계 유지).</p>
  </li>
  <li>
    <p>메타데이터: page, section, filename, lang. 답변에 근거 표시 용이.</p>
  </li>
  <li>
    <p>고유명사: 대소문자·하이픈·스네이크케이스 보존. 정규화 과도하게 하지 않기.</p>
  </li>
  <li>
    <p>캐싱: 빈번한 쿼리 번역/임베딩 결과 캐시.</p>
  </li>
  <li>
    <p>평가: 한-영 쌍으로 질의–정답 골드셋을 만들어 MRR@n, Hit@n, nDCG로 비교.</p>
  </li>
</ul>

<p>vLLM + gpt-oss 연결 포인트</p>

<ul>
  <li>
    <p>위 검색 결과(top-k 청크)를 user 메시지에 CONTEXT로 넣고 질문은 한국어로 전달.</p>
  </li>
  <li>
    <p>답변은 한국어로 강제, 필요한 부분만 영어 원문 인용.</p>
  </li>
  <li>
    <p>함수툴(예: search_docs)을 한 개 제공해 “모델이 필요 시 호출”하도록하면 자동 루프(에이전트 스타일)도 재현 가능.</p>
  </li>
</ul>

<p>요약</p>

<ul>
  <li>가능하고, 실전에서는 “다국어 임베딩 + 번역 병행 + 리랭킹”이 안정적입니다. 이미 구축된 영어 코퍼스에 최소 변경으로 한글 질의를 붙일 수 있고, gpt-oss 서빙과도 잘 맞습니다.</li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        


  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#llm" class="page__taxonomy-item p-category" rel="tag">llm</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2025-11-04T09:34:19+09:00">2025-11-04</time></p>

      </footer>

      

      
  <nav class="pagination">
    
      <a href="/llm/python-systemfile-tools/" class="pagination--pager" title="Python Tool 작성">Previous</a>
    
    
      <a href="/comfyui/comfyui-debug-node-refactor/" class="pagination--pager" title="ComfyUI Debug 노드 리팩터링">Next</a>
    
  </nav>


    </div>

    
  </article>

  
  
    
<div class="page__related">
  
  <h2 class="page__related-title">You may also enjoy</h2>
  <div class="grid__wrapper">
    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/2025-08-19-python-repr-vs-str/" rel="permalink">
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <!-- hvppyarchive: remove date icon
        <i class="far fa-calendar-alt" aria-hidden="true"></i> 
        -->
        
        <time datetime="2025-11-04T12:48:16+09:00">2025-11-04</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">
</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/comfyui/comfyui-debug-node-refactor/" rel="permalink">ComfyUI Debug 노드 리팩터링
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <!-- hvppyarchive: remove date icon
        <i class="far fa-calendar-alt" aria-hidden="true"></i> 
        -->
        
        <time datetime="2025-11-04T12:48:12+09:00">2025-11-04</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">

</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/llm/python-systemfile-tools/" rel="permalink">Python Tool 작성
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <!-- hvppyarchive: remove date icon
        <i class="far fa-calendar-alt" aria-hidden="true"></i> 
        -->
        
        <time datetime="2025-10-31T11:43:50+09:00">2025-10-31</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">

</p>
  </article>
</div>

    
      
      



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/comfyui/comfyui-connection-color/" rel="permalink">ComfyUI 타입 색상 추가
</a>
      
    </h2>
    

  <p class="page__meta">
    
      
      <span class="page__meta-date">
        <!-- hvppyarchive: remove date icon
        <i class="far fa-calendar-alt" aria-hidden="true"></i> 
        -->
        
        <time datetime="2025-10-30T16:07:05+09:00">2025-10-30</time>
      </span>
    

    

    
  </p>


    <p class="archive__item-excerpt" itemprop="description">

</p>
  </article>
</div>

    
  </div>
</div>

  
  
</div>

      
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="Enter your search term..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        

<div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
    

    
  </ul>
</div>


<div class="page__footer-copyright">&copy; 2013 - 2025 <a href="https://hvppyarchive.github.io">ARCHIVE</a>. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/jekyll-themes/minimal-mistakes/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>






  </body>
</html>
